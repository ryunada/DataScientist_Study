{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c6ef1e4",
   "metadata": {},
   "source": [
    "# 시계열 데이터(Time Series Data) 다루기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7ab7eb",
   "metadata": {},
   "source": [
    "## II. 전염병 예측_V2[ 시계열 데이터 일반화]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4443ba",
   "metadata": {},
   "source": [
    "- 문제 정의\n",
    "  - 3일 동안의 확진자 수 추이를 보고 다음날의 확진자 수를 예측\n",
    "\n",
    "\n",
    "```python\n",
    "from keras.models import Sequential\n",
    "from keras.layers import SimpleRNN, Dense, GRU, LSTM, Bidirectional, Conv1D, MaxPooling1D\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "```\n",
    "\n",
    "<details>\n",
    "  <summary>Library</summary>\n",
    "\n",
    "\n",
    "\n",
    "- from keras.models import Sequential\n",
    "  - 케라스의 모델 도구(models)중 시퀀셜 모델을 불러오는 명령어\n",
    "- from keras.layers import SimpleRNN, Dense, GRU, LSTM, Bidirectional, Conv1D, MaxPooling1D\n",
    "  - 레이어 도구(layers)중 SimpleRNN과 Dense도구등을 불러오는 명령어\n",
    "- from sklearn.prerprocessing import MinMaxSacler\n",
    "  - 데이터를 정규화하기 위한 MinMaaxScaler 함수를 불러오는 명령어\n",
    "- from sklearn.metrics import mean_squared_error\n",
    "  - 결과의 정확도를 계산하기 위한 함수인 mean_squared_error를 불러오는 명령어\n",
    "- from sklearn.model_selection import train_test_split\n",
    "  - 데이터를 훈련데이터와 검증 데이터로 나누는 명령어\n",
    "- import math\n",
    "  - 수학 계산을 도와주는 math 라이브러리\n",
    "- import numpy as np\n",
    "  - 수학 계산 라이브러리 numpy를 불러오고 np로 줄여서 사용\n",
    "- import matplotlib.pyplot as plt\n",
    "  - 그래프 라이브러리중 pyplot 라이브러리 사용\n",
    "  - plt로 줄여서 사용\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81389b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-14 16:01:35.307201: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import SimpleRNN, Dense, GRU, LSTM, Bidirectional, Conv1D, MaxPooling1D\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f219702",
   "metadata": {},
   "source": [
    "### 1. 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fd524f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ryu/Desktop/Desktop ryuseungho's MacBook Air/Data Scientist/Study_Organization/DeepLearning/RNN(LSTM,GRU)\n",
      "/Users/ryu/Desktop/Desktop ryuseungho's MacBook Air/Data Scientist/Study_Organization/DeepLearning/RNN(LSTM,GRU)/Data\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "os.chdir(\"/Users/ryu/Desktop/Desktop ryuseungho's MacBook Air/Data Scientist/Study_Organization/DeepLearning/RNN(LSTM,GRU)/Data\")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da557180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# github 저장소에서 데이터 불러오기\n",
    "# !git clone https://github.com/yhlee1627/deeplearning.git\n",
    "# git이 안될경우 주소에서 다운로드\n",
    "\n",
    "df = pd.read_csv('./deeplearning/corona_daily.csv', usecols = [3], engine = 'python', skipfooter = 3)\n",
    "dataframe = df.values\n",
    "dataframe = df.astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab8d2af",
   "metadata": {},
   "source": [
    "### 2. 데이터 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddbc095",
   "metadata": {},
   "source": [
    "#### 2-1. 데이터 분할\n",
    "- 각 분할에 사용할 샘플 개수 계산\n",
    "    - Train Data : 50%\n",
    "    - Val Data : 25%\n",
    "    - Test Data : 25%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecf5f371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data 개수 : 56\n",
      "Val Data 개수 : 28\n",
      "Test Data 개수 : 28\n"
     ]
    }
   ],
   "source": [
    "train_samples_n = int(0.5 * len(df))\n",
    "val_samples_n = int(0.25 * len(df))\n",
    "test_samples_n = len(df) - train_samples_n - val_samples_n\n",
    "print(f\"Train Data 개수 : {train_samples_n}\")\n",
    "print(f\"Val Data 개수 : {val_samples_n}\")\n",
    "print(f\"Test Data 개수 : {test_samples_n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcded04",
   "metadata": {},
   "source": [
    "### 2-2. 데이터 정규화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c03e566d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Confirmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>11190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>11206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>11225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>11265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>11344</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>112 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Confirmed\n",
       "0           24\n",
       "1           24\n",
       "2           27\n",
       "3           27\n",
       "4           28\n",
       "..         ...\n",
       "107      11190\n",
       "108      11206\n",
       "109      11225\n",
       "110      11265\n",
       "111      11344\n",
       "\n",
       "[112 rows x 1 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9acc41ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시계열은 스케일이 각각 다르므로 독립적으로 정규화(평균과 표준편차 사용)\n",
    "mean = df[:train_samples_n].mean(axis = 0)\n",
    "df -= mean\n",
    "std = df[:train_samples_n].std(axis = 0)\n",
    "df /= std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c04c52a",
   "metadata": {},
   "source": [
    "### 2-3. 시퀀스 데이터 준비 및 분할"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a552158",
   "metadata": {},
   "source": [
    "- 3일치의 데이터를 사용하여 4번째 날짜의 값을 예측\n",
    "<img src='https://p.ipic.vip/0wzzyd.png' width=50%>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00ef3c1",
   "metadata": {},
   "source": [
    "- 과정\n",
    "\n",
    "  - I. 현재 데이터의 시간 단위(1일)와 예측하려는 시간 단위 (1일)을 통일\n",
    "    - $sampling\\_rate = 1$\n",
    "  - II. 사용할 데이터의 기간 정의(3일)\n",
    "    - $sequenc\\_length = 3$\n",
    "  - III. 예측 시점 하루 뒤\n",
    "    - $delay = sampling\\_rate * (sequence\\_length + 1 - 1)$\n",
    "\n",
    "<img src='https://p.ipic.vip/cg3n6r.png'>\n",
    "\n",
    "- timeseries_dataset_from_array() : 중복된 데이터 때문에 생기는 메모리 낭비를 줄여줌\n",
    "\n",
    "<details>\n",
    "<summary>Option</summary>\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "tf.keras.utils.timeseries_dataset_from_array(\n",
    "        data,\n",
    "        targets,\n",
    "        sampling_rate = 1,\n",
    "\t\t\t  sequence_length,\n",
    "        delay = sampling_rate * (sequence_length + 24 - 1)      \n",
    "        sequence_stride = 1,\n",
    "        batch_size = 128,\n",
    "        shuffle = Fasle,\n",
    "        seed None,\n",
    "        start_index = None,\n",
    "        end_index = None\n",
    "    )\n",
    "```\n",
    "\n",
    "\n",
    "| Option                       |              Explanation              |\n",
    "| :--------------------------- | :-----------------------------------: |\n",
    "| data                         |      타깃 데이터를 제외한 데이터      |\n",
    "| targets                      |              타깃 데이터              |\n",
    "| sampling_rate                |         시퀀스 데이터의 단위          |\n",
    "| sequence_length              |  훈련에 사용할 시퀀스 데이터의 길이   |\n",
    "| delay                        |         예측하고자 하는 시점          |\n",
    "| sequence_stride(default = 1) |         연속 시계열 간의 거리         |\n",
    "| batch_size(default = 128)    | 각 배치의 시계열 샘플 수(마지막 제외) |\n",
    "| shuffle                      |        출력 샘플을 섞을지 말지        |\n",
    "| seed                         |                고정값                 |\n",
    "| start_index                  |   사용할 데이터의 시작 인덱스 위치    |\n",
    "| end_index                    |    사용할 데이터의 끝 인덱스 위치     |\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6d79795",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-14 16:01:45.406085: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "sampling_rate = 1  \n",
    "sequence_length = 3                                    # 이전 3일의 데이터 사용\n",
    "delay = sampling_rate * (sequence_length + 1 - 1)      # 하루 뒤 \n",
    "batch_size = 54                                        # 이 데이터에 최대 batch_size = 54\n",
    "\n",
    "# Training Data : 0 ~ train_samples_n\n",
    "train_dataset = keras.utils.timeseries_dataset_from_array(\n",
    "    df[:-delay],\n",
    "    targets = df[delay:],\n",
    "    sampling_rate = sampling_rate,\n",
    "    sequence_length = sequence_length,\n",
    "#     shuffle = True,\n",
    "    batch_size = batch_size,\n",
    "    start_index = 0,\n",
    "    end_index = train_samples_n\n",
    ")\n",
    "\n",
    "# Validation Data : train_samples_n ~ train_samples_n + val_samples_n\n",
    "val_dataset = keras.utils.timeseries_dataset_from_array(\n",
    "    df[:-delay],\n",
    "    targets = df[delay:],\n",
    "    sampling_rate = sampling_rate,\n",
    "    sequence_length = sequence_length,\n",
    "#     shuffle = True,\n",
    "    batch_size = batch_size,\n",
    "    start_index = train_samples_n,\n",
    "    end_index = train_samples_n + val_samples_n\n",
    ")\n",
    "\n",
    "# Test Data : train_samples_n + val_sampes_n + End\n",
    "test_dataset = keras.utils.timeseries_dataset_from_array(\n",
    "    df[:-delay],\n",
    "    targets = df[delay:],\n",
    "    sampling_rate = sampling_rate,\n",
    "    sequence_length = sequence_length,\n",
    "#     shuffle = True,\n",
    "    batch_size = batch_size,\n",
    "    start_index = train_samples_n + val_samples_n\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cdb77016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "샘플 크기 : (54, 3, 1)\n",
      "타깃 크기 : (54, 1)\n"
     ]
    }
   ],
   "source": [
    "for samples, targets in train_dataset:\n",
    "    print(f\"샘플 크기 : {samples.shape}\")\n",
    "    print(f\"타깃 크기 : {targets.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76f813d",
   "metadata": {},
   "source": [
    "### 3. 모델 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d62255",
   "metadata": {},
   "source": [
    "#### 3-1. 순환 신경망(RNN; Recurrent Neural Network)\n",
    "\n",
    "- 순차적인 데이터(Sequence Data)를 처리하기 위한 인공 신경망\n",
    "  - 어떤 특정 부분이 반복되는 구조에서 순서를 학습하기에 효과적\n",
    "  - 시계열 데이터, 자연어 등에 적용 가능\n",
    "  - 기존 Neural Network와 달리 '기억(Hidden State)'을 가지고 있음\n",
    "  - 은닉 계층 안에 하나 이상의 순환 계층을 갖는 신경망 구조\n",
    "    - 이전 단계의 출력 값이 현재 단계의 입력 값으로 다시 들어가는 반복 구조\n",
    "    - 가중치가 모든 타임 스텝에서 공유됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cf057f",
   "metadata": {},
   "source": [
    "\n",
    "<details>\n",
    "<summary>RNN Process</summary>\n",
    "\n",
    "\n",
    "\n",
    "<img src='https://p.ipic.vip/2bdtgi.png'>\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79bbeff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 3, 1)]            0         \n",
      "                                                                 \n",
      " simple_rnn (SimpleRNN)      (None, 16)                288       \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 305\n",
      "Trainable params: 305\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = keras.Input(shape = (sequence_length, df.shape[-1]))\n",
    "x = keras.layers.SimpleRNN(16, recurrent_dropout = 0.25)(inputs) \n",
    "outputs = keras.layers.Dense(1)(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d740980",
   "metadata": {},
   "source": [
    "####  3-2. LSTM ; Long Short Term Memory\n",
    "\n",
    "- RNN 의 **장기의존성 문제**와 **기울기 소실 문제**를 해결한 알고리즘\n",
    "\n",
    "  - 가중치 행렬 $W$의 행렬 곱 연산이 그레이디언트 경로에 나타나지 않도록 구조 변경\n",
    "\n",
    "- 기존 RNN에 장기 기억 셀(Cell State)을 추가함\n",
    "\n",
    "  - $c_t$를 연결하는 경로에는 가중치 행렬 $W$의 행렬 곱 연산이 없음\n",
    "\n",
    "    <img src='https://p.ipic.vip/hiuty3.png' width=80%>\n",
    "\n",
    "- 장기 기억 셀 연산에 사용되는 게이트 추가\n",
    "\n",
    "  - Forget Gate($f_t$) : 과거의 정보를 얼마나 유지할 것인가?\n",
    "  - Input Gate($i_t$) : 새로 입력된 정보를 얼만큼 활용할 것인가?\n",
    "  - Output Gate($o_t$) : Cell State 나온 정보를 얼마나 출력할 것인가?\n",
    "\n",
    "<details>\n",
    "<summary>LSTM Process</summary>\n",
    "<img src='https://p.ipic.vip/elqu30.png'>\n",
    "\n",
    "\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64a7a8ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 3, 1)]            0         \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 32)                4352      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,385\n",
      "Trainable params: 4,385\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = keras.Input(shape = (sequence_length, df.shape[-1]))\n",
    "x = keras.layers.LSTM(32, recurrent_dropout = 0.25)(inputs)\n",
    "outputs = keras.layers.Dense(1)(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdbd6ab",
   "metadata": {},
   "source": [
    "#### 3-3. GRU ; Gated Recurrent Unit\n",
    "\n",
    "- LSTM의 장점을 유지하면서 게이트 구조를 단순하게 만든 순환 신경망\n",
    "  - 업데이트 게이트(Update Gate) = Forget Gate + Input Gate\n",
    "    - 과거의 기억중 사용할 정보의 양과 현 시점의 입력 정보 중 사용할 정보 수집\n",
    "  - 리셋 게이트(Reset Gate)\n",
    "    - 현 시점의 입력 정보 중 새로운 정보를 추가할 때, 과거의 기억 중 필요한 정보의 양 계산\n",
    "  - 장기 기억 셀(Cell State)을 삭제\n",
    "    - 은닉 상태($h_{t-1}$)가 장기 기억과 단기 기억 모두를 기억하도록 함\n",
    "  - 출력 게이트가 존재하지 않음\n",
    "    - 전체 상태 벡터가 매 타임 스텝마다 출력\n",
    "\n",
    "<details>\n",
    "<summary>GRU Process</summary>\n",
    "<img src='https://p.ipic.vip/6obmtd.png'>\n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e251933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 3, 1)]            0         \n",
      "                                                                 \n",
      " gru (GRU)                   (None, 32)                3360      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,393\n",
      "Trainable params: 3,393\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = keras.Input(shape = (sequence_length, df.shape[-1]))\n",
    "x = keras.layers.GRU(32, recurrent_dropout = 0.25)(inputs)\n",
    "outputs = keras.layers.Dense(1)(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be392b0",
   "metadata": {},
   "source": [
    "#### 3-4. 양방향(Bidirectional) LSTM/GRU\n",
    "\n",
    "- 양방향 순환 층(Bidirectional Recurrent Layer)\n",
    "  - 순환 네트워크에 같은 정보를 다른 방향으로 주입하여 정확도를 높이고 기억을 좀 더 오래 유지\n",
    "  - 이전의 층이 전체 출력 시퀀스를 반환해야 함 [ return_sequence = True ]\n",
    "\n",
    "<details>\n",
    "<summary>양방향 LSTM/GRU</summary>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "017cda72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 3, 1)]            0         \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 32)               2304      \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,337\n",
      "Trainable params: 2,337\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = keras.Input(shape = (sequence_length, df.shape[-1]))\n",
    "x = keras.layers.Bidirectional(keras.layers.LSTM(16))(inputs)\n",
    "outputs = keras.layers.Dense(1)(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c582d6",
   "metadata": {},
   "source": [
    "#### 3-5. 1D CNN + LSTM/GRU\n",
    "\n",
    "<details>\n",
    "<summary>1D CNN + LSTM/GRU</summary>\n",
    "<img src='https://p.ipic.vip/bzx124.png'>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "def02473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_7 (InputLayer)        [(None, 3, 1)]            0         \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 3, 32)             64        \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPooling  (None, 1, 32)            0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 32)                8320      \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,417\n",
      "Trainable params: 8,417\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = keras.Input(shape = (sequence_length, df.shape[-1]))\n",
    "x = keras.layers.Conv1D(filters=32,\n",
    "               kernel_size=1,\n",
    "               strides=1,\n",
    "               activation='relu')(inputs)\n",
    "x = keras.layers.MaxPooling1D(pool_size = 3)(x)\n",
    "x = keras.layers.LSTM(32, recurrent_dropout = 0.25)(x)\n",
    "outputs = keras.layers.Dense(1)(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f54788f",
   "metadata": {},
   "source": [
    "### 4. 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d3638088",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.0201 - mae: 0.1140 - val_loss: 0.0554 - val_mae: 0.2349\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0134 - mae: 0.0933 - val_loss: 0.0386 - val_mae: 0.1961\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0126 - mae: 0.0937 - val_loss: 0.0316 - val_mae: 0.1772\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0134 - mae: 0.0842 - val_loss: 0.0302 - val_mae: 0.1733\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0121 - mae: 0.0908 - val_loss: 0.0285 - val_mae: 0.1683\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0116 - mae: 0.0857 - val_loss: 0.0282 - val_mae: 0.1675\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0115 - mae: 0.0869 - val_loss: 0.0278 - val_mae: 0.1663\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0114 - mae: 0.0857 - val_loss: 0.0277 - val_mae: 0.1660\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0113 - mae: 0.0858 - val_loss: 0.0275 - val_mae: 0.1653\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0112 - mae: 0.0852 - val_loss: 0.0273 - val_mae: 0.1649\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0111 - mae: 0.0851 - val_loss: 0.0271 - val_mae: 0.1642\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0110 - mae: 0.0845 - val_loss: 0.0269 - val_mae: 0.1637\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0109 - mae: 0.0844 - val_loss: 0.0267 - val_mae: 0.1630\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0108 - mae: 0.0838 - val_loss: 0.0266 - val_mae: 0.1625\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0106 - mae: 0.0836 - val_loss: 0.0262 - val_mae: 0.1616\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0105 - mae: 0.0828 - val_loss: 0.0261 - val_mae: 0.1611\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0104 - mae: 0.0829 - val_loss: 0.0257 - val_mae: 0.1598\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0103 - mae: 0.0816 - val_loss: 0.0255 - val_mae: 0.1592\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0103 - mae: 0.0828 - val_loss: 0.0247 - val_mae: 0.1568\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0103 - mae: 0.0797 - val_loss: 0.0251 - val_mae: 0.1581\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0105 - mae: 0.0852 - val_loss: 0.0238 - val_mae: 0.1539\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0106 - mae: 0.0779 - val_loss: 0.0243 - val_mae: 0.1554\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0103 - mae: 0.0838 - val_loss: 0.0233 - val_mae: 0.1521\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0099 - mae: 0.0781 - val_loss: 0.0232 - val_mae: 0.1521\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0097 - mae: 0.0806 - val_loss: 0.0226 - val_mae: 0.1499\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0096 - mae: 0.0780 - val_loss: 0.0224 - val_mae: 0.1493\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0094 - mae: 0.0791 - val_loss: 0.0218 - val_mae: 0.1474\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0093 - mae: 0.0772 - val_loss: 0.0216 - val_mae: 0.1466\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0092 - mae: 0.0781 - val_loss: 0.0210 - val_mae: 0.1447\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0091 - mae: 0.0761 - val_loss: 0.0208 - val_mae: 0.1438\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0090 - mae: 0.0773 - val_loss: 0.0201 - val_mae: 0.1413\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0090 - mae: 0.0748 - val_loss: 0.0199 - val_mae: 0.1408\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0089 - mae: 0.0773 - val_loss: 0.0191 - val_mae: 0.1379\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0089 - mae: 0.0732 - val_loss: 0.0190 - val_mae: 0.1374\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0089 - mae: 0.0778 - val_loss: 0.0184 - val_mae: 0.1352\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0088 - mae: 0.0721 - val_loss: 0.0178 - val_mae: 0.1330\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0087 - mae: 0.0765 - val_loss: 0.0178 - val_mae: 0.1331\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0085 - mae: 0.0716 - val_loss: 0.0163 - val_mae: 0.1275\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0084 - mae: 0.0744 - val_loss: 0.0176 - val_mae: 0.1322\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0082 - mae: 0.0712 - val_loss: 0.0144 - val_mae: 0.1197\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0081 - mae: 0.0729 - val_loss: 0.0182 - val_mae: 0.1345\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0080 - mae: 0.0709 - val_loss: 0.0120 - val_mae: 0.1091\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0079 - mae: 0.0721 - val_loss: 0.0194 - val_mae: 0.1390\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0079 - mae: 0.0703 - val_loss: 0.0106 - val_mae: 0.1028\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0078 - mae: 0.0716 - val_loss: 0.0182 - val_mae: 0.1345\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0077 - mae: 0.0692 - val_loss: 0.0112 - val_mae: 0.1056\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0077 - mae: 0.0709 - val_loss: 0.0156 - val_mae: 0.1245\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0076 - mae: 0.0678 - val_loss: 0.0122 - val_mae: 0.1102\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0075 - mae: 0.0705 - val_loss: 0.0133 - val_mae: 0.1150\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0075 - mae: 0.0662 - val_loss: 0.0129 - val_mae: 0.1132\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0074 - mae: 0.0701 - val_loss: 0.0115 - val_mae: 0.1071\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0073 - mae: 0.0652 - val_loss: 0.0132 - val_mae: 0.1145\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0072 - mae: 0.0692 - val_loss: 0.0101 - val_mae: 0.1001\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0071 - mae: 0.0646 - val_loss: 0.0134 - val_mae: 0.1154\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0070 - mae: 0.0680 - val_loss: 0.0086 - val_mae: 0.0928\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0069 - mae: 0.0641 - val_loss: 0.0138 - val_mae: 0.1173\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0068 - mae: 0.0672 - val_loss: 0.0075 - val_mae: 0.0862\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0067 - mae: 0.0634 - val_loss: 0.0137 - val_mae: 0.1167\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0067 - mae: 0.0664 - val_loss: 0.0071 - val_mae: 0.0841\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0066 - mae: 0.0626 - val_loss: 0.0125 - val_mae: 0.1116\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0065 - mae: 0.0656 - val_loss: 0.0073 - val_mae: 0.0851\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0064 - mae: 0.0615 - val_loss: 0.0110 - val_mae: 0.1047\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0064 - mae: 0.0648 - val_loss: 0.0075 - val_mae: 0.0866\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0063 - mae: 0.0603 - val_loss: 0.0096 - val_mae: 0.0977\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0063 - mae: 0.0642 - val_loss: 0.0078 - val_mae: 0.0881\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0062 - mae: 0.0593 - val_loss: 0.0082 - val_mae: 0.0903\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0062 - mae: 0.0633 - val_loss: 0.0082 - val_mae: 0.0903\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0061 - mae: 0.0587 - val_loss: 0.0067 - val_mae: 0.0817\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0060 - mae: 0.0621 - val_loss: 0.0089 - val_mae: 0.0944\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0059 - mae: 0.0584 - val_loss: 0.0050 - val_mae: 0.0706\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0058 - mae: 0.0612 - val_loss: 0.0100 - val_mae: 0.1001\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0058 - mae: 0.0584 - val_loss: 0.0041 - val_mae: 0.0637\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0057 - mae: 0.0604 - val_loss: 0.0095 - val_mae: 0.0972\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0056 - mae: 0.0577 - val_loss: 0.0044 - val_mae: 0.0664\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0056 - mae: 0.0595 - val_loss: 0.0079 - val_mae: 0.0887\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0055 - mae: 0.0564 - val_loss: 0.0049 - val_mae: 0.0696\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0054 - mae: 0.0586 - val_loss: 0.0067 - val_mae: 0.0815\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0054 - mae: 0.0553 - val_loss: 0.0051 - val_mae: 0.0716\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0053 - mae: 0.0582 - val_loss: 0.0057 - val_mae: 0.0753\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0053 - mae: 0.0542 - val_loss: 0.0053 - val_mae: 0.0727\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0053 - mae: 0.0580 - val_loss: 0.0049 - val_mae: 0.0697\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0052 - mae: 0.0532 - val_loss: 0.0054 - val_mae: 0.0732\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0052 - mae: 0.0574 - val_loss: 0.0041 - val_mae: 0.0638\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0051 - mae: 0.0524 - val_loss: 0.0055 - val_mae: 0.0742\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0050 - mae: 0.0563 - val_loss: 0.0032 - val_mae: 0.0561\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0049 - mae: 0.0519 - val_loss: 0.0062 - val_mae: 0.0789\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0049 - mae: 0.0557 - val_loss: 0.0021 - val_mae: 0.0456\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0048 - mae: 0.0515 - val_loss: 0.0070 - val_mae: 0.0834\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0048 - mae: 0.0554 - val_loss: 0.0018 - val_mae: 0.0419\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0047 - mae: 0.0510 - val_loss: 0.0061 - val_mae: 0.0779\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0047 - mae: 0.0544 - val_loss: 0.0022 - val_mae: 0.0463\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0046 - mae: 0.0499 - val_loss: 0.0047 - val_mae: 0.0688\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0045 - mae: 0.0534 - val_loss: 0.0025 - val_mae: 0.0501\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0045 - mae: 0.0489 - val_loss: 0.0037 - val_mae: 0.0609\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0045 - mae: 0.0529 - val_loss: 0.0028 - val_mae: 0.0533\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0045 - mae: 0.0480 - val_loss: 0.0028 - val_mae: 0.0529\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0044 - mae: 0.0524 - val_loss: 0.0032 - val_mae: 0.0566\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0044 - mae: 0.0476 - val_loss: 0.0020 - val_mae: 0.0442\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0043 - mae: 0.0515 - val_loss: 0.0038 - val_mae: 0.0614\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0043 - mae: 0.0474 - val_loss: 0.0012 - val_mae: 0.0350\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer = 'rmsprop', loss = 'mse', metrics=['mae'])\n",
    "history = model.fit(train_dataset,\n",
    "                   epochs = 100,\n",
    "                   validation_data = val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706ca636",
   "metadata": {},
   "source": [
    "### 5. 고급 기법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f34d59",
   "metadata": {},
   "source": [
    "#### 5-1. 스태킹 순환 층(Stacking Recurrent Layer) \n",
    "\n",
    "\n",
    "- 모델의 표현 능력(Representational Power)을 증가 시킴  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e15d13c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 1.0992 - mae: 0.9836 - val_loss: 2.1423 - val_mae: 1.4632\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 1.0211 - mae: 0.9462 - val_loss: 1.9958 - val_mae: 1.4123\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.9689 - mae: 0.9203 - val_loss: 1.8757 - val_mae: 1.3691\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.9255 - mae: 0.8983 - val_loss: 1.7681 - val_mae: 1.3293\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.8861 - mae: 0.8778 - val_loss: 1.6672 - val_mae: 1.2908\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.8488 - mae: 0.8579 - val_loss: 1.5703 - val_mae: 1.2527\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.8123 - mae: 0.8380 - val_loss: 1.4761 - val_mae: 1.2146\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.7762 - mae: 0.8180 - val_loss: 1.3840 - val_mae: 1.1761\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.7402 - mae: 0.7975 - val_loss: 1.2936 - val_mae: 1.1370\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.7040 - mae: 0.7765 - val_loss: 1.2046 - val_mae: 1.0972\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.6677 - mae: 0.7548 - val_loss: 1.1168 - val_mae: 1.0565\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.6311 - mae: 0.7325 - val_loss: 1.0304 - val_mae: 1.0147\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.5943 - mae: 0.7094 - val_loss: 0.9452 - val_mae: 0.9719\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.5573 - mae: 0.6855 - val_loss: 0.8616 - val_mae: 0.9279\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.5202 - mae: 0.6608 - val_loss: 0.7796 - val_mae: 0.8827\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.4833 - mae: 0.6352 - val_loss: 0.6999 - val_mae: 0.8363\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.4466 - mae: 0.6089 - val_loss: 0.6229 - val_mae: 0.7890\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.4104 - mae: 0.5817 - val_loss: 0.5491 - val_mae: 0.7408\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.3749 - mae: 0.5539 - val_loss: 0.4792 - val_mae: 0.6920\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.3403 - mae: 0.5255 - val_loss: 0.4136 - val_mae: 0.6429\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.3068 - mae: 0.4966 - val_loss: 0.3528 - val_mae: 0.5938\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.2746 - mae: 0.4677 - val_loss: 0.2973 - val_mae: 0.5451\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.2440 - mae: 0.4386 - val_loss: 0.2473 - val_mae: 0.4971\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.2150 - mae: 0.4094 - val_loss: 0.2028 - val_mae: 0.4501\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.1880 - mae: 0.3802 - val_loss: 0.1637 - val_mae: 0.4045\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.1630 - mae: 0.3519 - val_loss: 0.1301 - val_mae: 0.3606\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.1401 - mae: 0.3248 - val_loss: 0.1016 - val_mae: 0.3186\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.1194 - mae: 0.2985 - val_loss: 0.0778 - val_mae: 0.2789\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.1010 - mae: 0.2736 - val_loss: 0.0584 - val_mae: 0.2416\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0848 - mae: 0.2500 - val_loss: 0.0429 - val_mae: 0.2070\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0709 - mae: 0.2274 - val_loss: 0.0307 - val_mae: 0.1751\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0590 - mae: 0.2058 - val_loss: 0.0214 - val_mae: 0.1461\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0492 - mae: 0.1855 - val_loss: 0.0144 - val_mae: 0.1200\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0411 - mae: 0.1675 - val_loss: 0.0094 - val_mae: 0.0969\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0347 - mae: 0.1507 - val_loss: 0.0059 - val_mae: 0.0769\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0297 - mae: 0.1354 - val_loss: 0.0036 - val_mae: 0.0599\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0259 - mae: 0.1233 - val_loss: 0.0021 - val_mae: 0.0459\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0230 - mae: 0.1141 - val_loss: 0.0012 - val_mae: 0.0349\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0208 - mae: 0.1081 - val_loss: 7.2381e-04 - val_mae: 0.0267\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0192 - mae: 0.1042 - val_loss: 4.5536e-04 - val_mae: 0.0211\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0180 - mae: 0.1009 - val_loss: 3.2732e-04 - val_mae: 0.0177\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0171 - mae: 0.0984 - val_loss: 2.8329e-04 - val_mae: 0.0164\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0163 - mae: 0.0964 - val_loss: 2.9614e-04 - val_mae: 0.0167\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0156 - mae: 0.0948 - val_loss: 3.5711e-04 - val_mae: 0.0183\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0151 - mae: 0.0933 - val_loss: 4.6682e-04 - val_mae: 0.0210\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0146 - mae: 0.0920 - val_loss: 6.2911e-04 - val_mae: 0.0245\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0141 - mae: 0.0906 - val_loss: 8.4732e-04 - val_mae: 0.0286\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0136 - mae: 0.0894 - val_loss: 0.0011 - val_mae: 0.0330\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0132 - mae: 0.0881 - val_loss: 0.0015 - val_mae: 0.0376\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0128 - mae: 0.0869 - val_loss: 0.0018 - val_mae: 0.0424\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0124 - mae: 0.0858 - val_loss: 0.0023 - val_mae: 0.0472\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0120 - mae: 0.0847 - val_loss: 0.0028 - val_mae: 0.0521\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0116 - mae: 0.0837 - val_loss: 0.0033 - val_mae: 0.0569\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0113 - mae: 0.0827 - val_loss: 0.0039 - val_mae: 0.0617\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0109 - mae: 0.0818 - val_loss: 0.0045 - val_mae: 0.0665\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0106 - mae: 0.0808 - val_loss: 0.0051 - val_mae: 0.0712\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0103 - mae: 0.0799 - val_loss: 0.0058 - val_mae: 0.0758\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0100 - mae: 0.0790 - val_loss: 0.0066 - val_mae: 0.0804\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0097 - mae: 0.0781 - val_loss: 0.0073 - val_mae: 0.0850\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0094 - mae: 0.0772 - val_loss: 0.0081 - val_mae: 0.0894\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0092 - mae: 0.0763 - val_loss: 0.0089 - val_mae: 0.0939\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0089 - mae: 0.0754 - val_loss: 0.0097 - val_mae: 0.0980\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0087 - mae: 0.0746 - val_loss: 0.0108 - val_mae: 0.1031\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0084 - mae: 0.0737 - val_loss: 0.0111 - val_mae: 0.1047\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0082 - mae: 0.0733 - val_loss: 0.0140 - val_mae: 0.1178\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0081 - mae: 0.0713 - val_loss: 0.0097 - val_mae: 0.0977\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0083 - mae: 0.0763 - val_loss: 0.0212 - val_mae: 0.1451\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0087 - mae: 0.0697 - val_loss: 0.0112 - val_mae: 0.1053\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0081 - mae: 0.0753 - val_loss: 0.0186 - val_mae: 0.1359\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0076 - mae: 0.0687 - val_loss: 0.0151 - val_mae: 0.1220\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0074 - mae: 0.0707 - val_loss: 0.0185 - val_mae: 0.1354\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0072 - mae: 0.0683 - val_loss: 0.0174 - val_mae: 0.1311\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0070 - mae: 0.0690 - val_loss: 0.0197 - val_mae: 0.1396\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0069 - mae: 0.0675 - val_loss: 0.0192 - val_mae: 0.1378\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0068 - mae: 0.0679 - val_loss: 0.0212 - val_mae: 0.1451\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0067 - mae: 0.0666 - val_loss: 0.0207 - val_mae: 0.1433\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0066 - mae: 0.0671 - val_loss: 0.0231 - val_mae: 0.1512\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0066 - mae: 0.0654 - val_loss: 0.0221 - val_mae: 0.1479\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0065 - mae: 0.0670 - val_loss: 0.0250 - val_mae: 0.1574\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0066 - mae: 0.0640 - val_loss: 0.0238 - val_mae: 0.1537\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0068 - mae: 0.0692 - val_loss: 0.0249 - val_mae: 0.1570\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0071 - mae: 0.0629 - val_loss: 0.0294 - val_mae: 0.1707\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0074 - mae: 0.0727 - val_loss: 0.0208 - val_mae: 0.1433\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0074 - mae: 0.0631 - val_loss: 0.0355 - val_mae: 0.1877\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0074 - mae: 0.0719 - val_loss: 0.0201 - val_mae: 0.1408\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0069 - mae: 0.0627 - val_loss: 0.0359 - val_mae: 0.1887\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0068 - mae: 0.0682 - val_loss: 0.0223 - val_mae: 0.1486\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0065 - mae: 0.0620 - val_loss: 0.0349 - val_mae: 0.1862\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0064 - mae: 0.0655 - val_loss: 0.0244 - val_mae: 0.1555\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0062 - mae: 0.0613 - val_loss: 0.0350 - val_mae: 0.1863\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0062 - mae: 0.0646 - val_loss: 0.0257 - val_mae: 0.1596\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0062 - mae: 0.0608 - val_loss: 0.0359 - val_mae: 0.1887\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0063 - mae: 0.0650 - val_loss: 0.0263 - val_mae: 0.1614\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0063 - mae: 0.0603 - val_loss: 0.0373 - val_mae: 0.1924\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0065 - mae: 0.0669 - val_loss: 0.0264 - val_mae: 0.1617\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0066 - mae: 0.0600 - val_loss: 0.0387 - val_mae: 0.1960\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0067 - mae: 0.0683 - val_loss: 0.0265 - val_mae: 0.1621\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0066 - mae: 0.0598 - val_loss: 0.0393 - val_mae: 0.1976\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0066 - mae: 0.0674 - val_loss: 0.0271 - val_mae: 0.1639\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0063 - mae: 0.0594 - val_loss: 0.0393 - val_mae: 0.1976\n"
     ]
    }
   ],
   "source": [
    "# 스태킹\n",
    "inputs = keras.Input(shape = (sequence_length, df.shape[-1]))\n",
    "x = keras.layers.GRU(32, return_sequences = True)(inputs)\n",
    "x = keras.layers.GRU(32)(x)\n",
    "x = keras.layers.Dense(1)(x)\n",
    "outputs = keras.layers.Dense(1)(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "model.compile(optimizer = 'rmsprop', loss = 'mse',  metrics = ['mae'])\n",
    "history = model.fit(train_dataset,\n",
    "                   epochs = 100,\n",
    "                   validation_data = val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235cb154",
   "metadata": {},
   "source": [
    "#### 5-2. 순환 드롭아웃(Recurrent Dropout) \n",
    "\n",
    "- 드롭아웃의 한 종류로 순환 층에서 과대적합을 방지하기 위해 사용\n",
    "\n",
    "- 조건 : 모든 중간층은 전체 출력 시퀀스를 반환해야 함 [return_sequence = True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "53224da4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 1.3941 - mae: 1.1030 - val_loss: 2.4479 - val_mae: 1.5641\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 1.1501 - mae: 1.0000 - val_loss: 2.1130 - val_mae: 1.4531\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 1.0521 - mae: 0.9518 - val_loss: 1.8699 - val_mae: 1.3670\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.9044 - mae: 0.8864 - val_loss: 1.6486 - val_mae: 1.2836\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.8522 - mae: 0.8540 - val_loss: 1.4573 - val_mae: 1.2068\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.8103 - mae: 0.8300 - val_loss: 1.2914 - val_mae: 1.1360\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.6853 - mae: 0.7623 - val_loss: 1.1363 - val_mae: 1.0656\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.6613 - mae: 0.7459 - val_loss: 1.0022 - val_mae: 1.0008\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.5720 - mae: 0.6941 - val_loss: 0.8794 - val_mae: 0.9375\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.5089 - mae: 0.6498 - val_loss: 0.7781 - val_mae: 0.8818\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.4599 - mae: 0.6135 - val_loss: 0.6765 - val_mae: 0.8223\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.3642 - mae: 0.5531 - val_loss: 0.5879 - val_mae: 0.7665\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.3225 - mae: 0.5117 - val_loss: 0.5069 - val_mae: 0.7117\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.3316 - mae: 0.5159 - val_loss: 0.4238 - val_mae: 0.6508\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.2481 - mae: 0.4364 - val_loss: 0.3574 - val_mae: 0.5976\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.2166 - mae: 0.3949 - val_loss: 0.3096 - val_mae: 0.5563\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.1808 - mae: 0.3837 - val_loss: 0.2426 - val_mae: 0.4924\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.1932 - mae: 0.3716 - val_loss: 0.2014 - val_mae: 0.4486\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.1556 - mae: 0.3499 - val_loss: 0.1582 - val_mae: 0.3977\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.1402 - mae: 0.3089 - val_loss: 0.1196 - val_mae: 0.3457\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.1255 - mae: 0.2901 - val_loss: 0.0928 - val_mae: 0.3045\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0961 - mae: 0.2509 - val_loss: 0.0743 - val_mae: 0.2725\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0961 - mae: 0.2512 - val_loss: 0.0534 - val_mae: 0.2311\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0836 - mae: 0.2180 - val_loss: 0.0489 - val_mae: 0.2212\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0760 - mae: 0.2218 - val_loss: 0.0362 - val_mae: 0.1903\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.1004 - mae: 0.2621 - val_loss: 0.0238 - val_mae: 0.1543\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0516 - mae: 0.1747 - val_loss: 0.0145 - val_mae: 0.1204\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0483 - mae: 0.1763 - val_loss: 0.0074 - val_mae: 0.0862\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0579 - mae: 0.1914 - val_loss: 0.0061 - val_mae: 0.0783\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0741 - mae: 0.2017 - val_loss: 0.0064 - val_mae: 0.0798\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0796 - mae: 0.2229 - val_loss: 0.0014 - val_mae: 0.0368\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0605 - mae: 0.2073 - val_loss: 0.0040 - val_mae: 0.0633\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0471 - mae: 0.1786 - val_loss: 0.0065 - val_mae: 0.0804\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0683 - mae: 0.2113 - val_loss: 0.0053 - val_mae: 0.0725\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0576 - mae: 0.2023 - val_loss: 0.0024 - val_mae: 0.0490\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0421 - mae: 0.1660 - val_loss: 0.0056 - val_mae: 0.0748\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0403 - mae: 0.1672 - val_loss: 0.0051 - val_mae: 0.0712\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0448 - mae: 0.1748 - val_loss: 0.0022 - val_mae: 0.0470\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0610 - mae: 0.2036 - val_loss: 6.2819e-04 - val_mae: 0.0249\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0493 - mae: 0.1886 - val_loss: 0.0044 - val_mae: 0.0662\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0582 - mae: 0.1993 - val_loss: 0.0041 - val_mae: 0.0636\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0488 - mae: 0.1723 - val_loss: 0.0037 - val_mae: 0.0604\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0548 - mae: 0.2014 - val_loss: 0.0013 - val_mae: 0.0352\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0397 - mae: 0.1505 - val_loss: 1.8984e-04 - val_mae: 0.0134\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0476 - mae: 0.1526 - val_loss: 0.0031 - val_mae: 0.0559\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0520 - mae: 0.1869 - val_loss: 0.0031 - val_mae: 0.0559\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0629 - mae: 0.2085 - val_loss: 0.0019 - val_mae: 0.0435\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0513 - mae: 0.1912 - val_loss: 0.0043 - val_mae: 0.0658\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0406 - mae: 0.1661 - val_loss: 0.0026 - val_mae: 0.0507\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0464 - mae: 0.1753 - val_loss: 9.5443e-04 - val_mae: 0.0306\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0455 - mae: 0.1658 - val_loss: 0.0108 - val_mae: 0.1039\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0455 - mae: 0.1636 - val_loss: 0.0025 - val_mae: 0.0498\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0660 - mae: 0.2045 - val_loss: 4.3774e-05 - val_mae: 0.0056\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0327 - mae: 0.1449 - val_loss: 0.0054 - val_mae: 0.0733\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0617 - mae: 0.1970 - val_loss: 0.0274 - val_mae: 0.1655\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0564 - mae: 0.1887 - val_loss: 0.0104 - val_mae: 0.1017\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0645 - mae: 0.1945 - val_loss: 0.0029 - val_mae: 0.0534\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0538 - mae: 0.1915 - val_loss: 0.0032 - val_mae: 0.0568\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0301 - mae: 0.1386 - val_loss: 0.0091 - val_mae: 0.0953\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0349 - mae: 0.1474 - val_loss: 0.0161 - val_mae: 0.1269\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0383 - mae: 0.1681 - val_loss: 0.0272 - val_mae: 0.1646\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0434 - mae: 0.1656 - val_loss: 0.0086 - val_mae: 0.0925\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0376 - mae: 0.1549 - val_loss: 0.0126 - val_mae: 0.1120\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0400 - mae: 0.1753 - val_loss: 0.0053 - val_mae: 0.0726\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0357 - mae: 0.1558 - val_loss: 0.0023 - val_mae: 0.0472\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0487 - mae: 0.1769 - val_loss: 0.0064 - val_mae: 0.0797\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0675 - mae: 0.1905 - val_loss: 0.0144 - val_mae: 0.1197\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0575 - mae: 0.1852 - val_loss: 0.0139 - val_mae: 0.1178\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0445 - mae: 0.1703 - val_loss: 0.0031 - val_mae: 0.0550\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0324 - mae: 0.1433 - val_loss: 0.0014 - val_mae: 0.0376\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0502 - mae: 0.1828 - val_loss: 0.0124 - val_mae: 0.1113\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0421 - mae: 0.1517 - val_loss: 0.0145 - val_mae: 0.1202\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0458 - mae: 0.1665 - val_loss: 0.0249 - val_mae: 0.1575\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0238 - mae: 0.1250 - val_loss: 0.0243 - val_mae: 0.1556\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0292 - mae: 0.1392 - val_loss: 0.0180 - val_mae: 0.1341\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0229 - mae: 0.1250 - val_loss: 0.0151 - val_mae: 0.1226\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0404 - mae: 0.1535 - val_loss: 0.0275 - val_mae: 0.1656\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0411 - mae: 0.1709 - val_loss: 0.0177 - val_mae: 0.1327\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0511 - mae: 0.1740 - val_loss: 0.0240 - val_mae: 0.1547\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0344 - mae: 0.1516 - val_loss: 0.0236 - val_mae: 0.1534\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0342 - mae: 0.1504 - val_loss: 0.0139 - val_mae: 0.1178\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0472 - mae: 0.1794 - val_loss: 0.0481 - val_mae: 0.2192\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0498 - mae: 0.1660 - val_loss: 0.0171 - val_mae: 0.1306\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0398 - mae: 0.1681 - val_loss: 0.0184 - val_mae: 0.1355\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0537 - mae: 0.1863 - val_loss: 0.0118 - val_mae: 0.1082\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0461 - mae: 0.1696 - val_loss: 0.0189 - val_mae: 0.1373\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0397 - mae: 0.1566 - val_loss: 0.0162 - val_mae: 0.1268\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0418 - mae: 0.1569 - val_loss: 0.0206 - val_mae: 0.1434\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0428 - mae: 0.1689 - val_loss: 0.0133 - val_mae: 0.1148\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0343 - mae: 0.1486 - val_loss: 0.0149 - val_mae: 0.1216\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0404 - mae: 0.1628 - val_loss: 0.0441 - val_mae: 0.2096\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0278 - mae: 0.1310 - val_loss: 0.0332 - val_mae: 0.1820\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0270 - mae: 0.1289 - val_loss: 0.0151 - val_mae: 0.1224\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0485 - mae: 0.1608 - val_loss: 0.0147 - val_mae: 0.1207\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0363 - mae: 0.1508 - val_loss: 0.0126 - val_mae: 0.1118\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0376 - mae: 0.1519 - val_loss: 0.0169 - val_mae: 0.1297\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0262 - mae: 0.1250 - val_loss: 0.0274 - val_mae: 0.1653\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0374 - mae: 0.1522 - val_loss: 0.0350 - val_mae: 0.1868\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0421 - mae: 0.1738 - val_loss: 0.0262 - val_mae: 0.1614\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0302 - mae: 0.1389 - val_loss: 0.0154 - val_mae: 0.1239\n"
     ]
    }
   ],
   "source": [
    "# 드롭아웃\n",
    "inputs = keras.Input(shape = (sequence_length, df.shape[-1]))\n",
    "x = keras.layers.GRU(32, recurrent_dropout = 0.25, return_sequences = True)(inputs)\n",
    "x = keras.layers.GRU(32, recurrent_dropout = 0.25)(x)\n",
    "x = keras.layers.Dropout(0.5)(x)\n",
    "outputs = keras.layers.Dense(1)(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "model.compile(optimizer = 'rmsprop', loss = 'mse', metrics=['mae'])\n",
    "history = model.fit(train_dataset,\n",
    "                   epochs = 100,\n",
    "                   validation_data = val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7854d4fc",
   "metadata": {},
   "source": [
    "#### 5-3. Early Stopping\n",
    "\n",
    "- 너무 많은 Epoch는 Overfitting을 일으킨다\n",
    "- 너무 적은 Epoch는 Underfitting을 일으킨다\n",
    "- Epoch를 많이 돌린 후, 특정 시점에서 멈춤\n",
    "  - monitor : 모델 학습을 멈추는 기준\n",
    "  - patience : 최소 반복수\n",
    "  - verbose : 자세한 정보 표시 모드 ( 0 or 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4a43ab4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.9084 - mae: 0.8887 - val_loss: 1.5987 - val_mae: 1.2640\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.7540 - mae: 0.8074 - val_loss: 1.3442 - val_mae: 1.1591\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.6517 - mae: 0.7481 - val_loss: 1.1566 - val_mae: 1.0751\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.5947 - mae: 0.7128 - val_loss: 1.0057 - val_mae: 1.0025\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.5408 - mae: 0.6724 - val_loss: 0.8727 - val_mae: 0.9339\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.4438 - mae: 0.6075 - val_loss: 0.7563 - val_mae: 0.8694\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.4189 - mae: 0.5907 - val_loss: 0.6498 - val_mae: 0.8059\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.3529 - mae: 0.5386 - val_loss: 0.5556 - val_mae: 0.7451\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.2998 - mae: 0.4908 - val_loss: 0.4746 - val_mae: 0.6887\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.2792 - mae: 0.4627 - val_loss: 0.4063 - val_mae: 0.6372\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.2293 - mae: 0.4209 - val_loss: 0.3318 - val_mae: 0.5759\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.2057 - mae: 0.3818 - val_loss: 0.2811 - val_mae: 0.5300\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.1749 - mae: 0.3564 - val_loss: 0.2351 - val_mae: 0.4847\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.1637 - mae: 0.3496 - val_loss: 0.1842 - val_mae: 0.4290\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.1523 - mae: 0.3233 - val_loss: 0.1461 - val_mae: 0.3821\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.1369 - mae: 0.3082 - val_loss: 0.1122 - val_mae: 0.3349\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0958 - mae: 0.2449 - val_loss: 0.0837 - val_mae: 0.2892\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.1060 - mae: 0.2708 - val_loss: 0.0632 - val_mae: 0.2512\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0714 - mae: 0.2148 - val_loss: 0.0544 - val_mae: 0.2331\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0762 - mae: 0.2157 - val_loss: 0.0575 - val_mae: 0.2397\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0616 - mae: 0.2006 - val_loss: 0.0455 - val_mae: 0.2133\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.1156 - mae: 0.2793 - val_loss: 0.0240 - val_mae: 0.1547\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0471 - mae: 0.1760 - val_loss: 0.0250 - val_mae: 0.1579\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0673 - mae: 0.1996 - val_loss: 0.0179 - val_mae: 0.1337\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0796 - mae: 0.2251 - val_loss: 0.0185 - val_mae: 0.1358\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0567 - mae: 0.1958 - val_loss: 0.0128 - val_mae: 0.1130\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0566 - mae: 0.1819 - val_loss: 0.0083 - val_mae: 0.0910\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0694 - mae: 0.2106 - val_loss: 0.0082 - val_mae: 0.0907\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0711 - mae: 0.2213 - val_loss: 0.0130 - val_mae: 0.1140\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0570 - mae: 0.1975 - val_loss: 0.0060 - val_mae: 0.0775\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0738 - mae: 0.2230 - val_loss: 0.0074 - val_mae: 0.0858\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0557 - mae: 0.1905 - val_loss: 0.0107 - val_mae: 0.1031\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0404 - mae: 0.1610 - val_loss: 0.0085 - val_mae: 0.0920\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0599 - mae: 0.1970 - val_loss: 0.0097 - val_mae: 0.0981\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0671 - mae: 0.1959 - val_loss: 0.0134 - val_mae: 0.1156\n",
      "Epoch 35: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Early Stopping\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "inputs = keras.Input(shape = (sequence_length, df.shape[-1]))\n",
    "x = keras.layers.GRU(32, recurrent_dropout = 0.5, return_sequences = True)(inputs)\n",
    "x = keras.layers.GRU(32, recurrent_dropout = 0.5)(x)\n",
    "x = keras.layers.Dropout(0.5)(x)\n",
    "outputs = keras.layers.Dense(1)(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
    "\n",
    "model.compile(optimizer = 'rmsprop', loss = 'mse', metrics=['mae'])\n",
    "history = model.fit(train_dataset,\n",
    "                   epochs = 100,\n",
    "                   validation_data = val_dataset,\n",
    "                   callbacks = [early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97f2e6a",
   "metadata": {},
   "source": [
    "### 6. 데이터 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fdf8fbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset에서 samples과 target 분리\n",
    "for train_samples, train_targets in train_dataset:\n",
    "    pass\n",
    "for val_samples, val_targets in val_dataset:\n",
    "    pass\n",
    "for test_samples, test_targets in test_dataset:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1ed3f587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n"
     ]
    }
   ],
   "source": [
    "# 모델을 적용한 출력값\n",
    "trainPredict = model.predict(train_samples)\n",
    "valPredict = model.predict(val_samples)\n",
    "testPredict = model.predict(test_samples)\n",
    "\n",
    "# * std + mean을 통하여 실제 값으로 변환\n",
    "# Training Data 예측값\n",
    "Train_Predict = list()\n",
    "for i in trainPredict:\n",
    "    Train_Predict.append(i * std + mean)\n",
    "\n",
    "# Training Data 타깃 값\n",
    "Train_Targets = list()\n",
    "for i in train_targets:\n",
    "    Train_Targets.append(i * std + mean)\n",
    "    \n",
    "# Validation Data 예측값\n",
    "Val_Predict = list()\n",
    "for i in valPredict:\n",
    "    Val_Predict.append(i * std + mean)\n",
    "\n",
    "# Validation Data 타깃 \n",
    "Val_Targets = list()\n",
    "for i in val_targets:\n",
    "    Val_Targets.append(i * std + mean)\n",
    "    \n",
    "# Test Data 예측값\n",
    "Test_Predict = list()\n",
    "for i in testPredict:\n",
    "    Test_Predict.append(i * std + mean)\n",
    "    \n",
    "# Test Data 타깃값\n",
    "Test_Targets = list()\n",
    "for i in test_targets:\n",
    "    Test_Targets.append(i * std + mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9860ffdb",
   "metadata": {},
   "source": [
    "### 7. 모델의 정확도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a79f436d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score : 255.92 RMSE\n",
      "Train Score : 137.97 RMSE\n",
      "Train Score : 180.13 RMSE\n"
     ]
    }
   ],
   "source": [
    "trainScore = math.sqrt(mean_squared_error(Train_Targets, Train_Predict))\n",
    "print(f\"Train Score : {trainScore:.2f} RMSE\")\n",
    "\n",
    "valScore = math.sqrt(mean_squared_error(Val_Targets, Val_Predict))\n",
    "print(f\"Train Score : {valScore:.2f} RMSE\")\n",
    "\n",
    "testScore = math.sqrt(mean_squared_error(Test_Targets, Test_Predict))\n",
    "print(f\"Train Score : {testScore:.2f} RMSE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d20dc9f",
   "metadata": {},
   "source": [
    "### 8. 시각화 실제 값과 예측값 그래프"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0c6673e3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjoAAAGdCAYAAAAbudkLAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABOlElEQVR4nO3deXxU9b3/8dfs2YcsJCEQIEBYgwioLC5QRdyQWr3Viqb21utSV6rWpZvWVrC2VdtS1/aq16V4eyvW+rNU3FBkNRBWWWUJkJCQZbLPTGa+vz8GRsOOJJxk8n4+HvMIOeeTyeccA3n7Pd/vOTZjjEFEREQkBtmtbkBERESkvSjoiIiISMxS0BEREZGYpaAjIiIiMUtBR0RERGKWgo6IiIjELAUdERERiVkKOiIiIhKznFY3YKVwOMzu3btJTk7GZrNZ3Y6IiIgcA2MMdXV15OTkYLcfecymSwed3bt3k5uba3UbIiIi8jWUlJTQq1evI9Z06aCTnJwMRE5USkqKxd2IiIjIsaitrSU3Nzf6e/xIunTQ2X+5KiUlRUFHRESkkzmWaSeajCwiIiIxS0FHREREYpaCjoiIiMQsBR0RERGJWQo6IiIiErMUdERERCRmKeiIiIhIzFLQERERkZiloCMiIiIxS0FHREREYpaCjoiIiMQsBR0RERGJWQo6IiIi0ua27m3gv176jNU7fZb20aWfXi4iIiJtq7Y5yKwPNvPCp1sJhgz1/iCzbxxnWT8KOiIiInLC/C0h3lyxi9/8ewN76wMATBzUnZ9eMtTSvhR0RERE5Gsp8zXz0YZyPlhfzqeb99IQCAHQLyORn00ZyjcGZ1rcoYKOiIiIHEZzMESZr5ndNU3sqmliZ3UT2ysb2F7VyPbKRqoaAq3qM5M93HhOP747ri9uZ8eYBqygIyIi0sWFwoYdVY1sKKvj89LayKuslpKqpiN+nc0GI3p14xuDMjl3cCbDclKw220nqetjo6AjIiLSRTT4W9i6t4FtlQ1srWhgS0U9G/fUs6WiHn9L+JBfE+9y0DM1npxu8fTsFkfvtET6pCfQOy2BPukJJMe5TvJRHB8FHRERkRjiawqyfEc1n22rYtveRvbW+/e9Aviagof9Oo/TTn5WEkOyUxjcI4UhPZIZlJVMWqIbm61jjdIcDwUdERGRTqQ5GKK81k+pr4k9dX4q6vyU1zVTUetnXWktG/bUYczhvz4t0U1eRiJ90xPp1z2R/MwkBmYlk5uWgKODXXZqCwo6IiIiHUSgJcyumia2VTawo7KRPbXNlO8LMxV1fvbUNlN5wATgQ8nLSGR0n1SG9kihe7KH9CQ33ZM8ZKbE4Y3/8lJTqL4ee1wcNmfsxoHYPTIREZEOrLohwOpdPtbs9rFml4+1u2spqWokfITRmP08Tjs9vHFkpsSRmeyhe7KHzOQ48jISGN0nje7JHgBMIEBg+3b8m1bh37yFhpId1Owpp2XPHlr27CHc2EjeW/8gbuDAdj5a6yjoiIiItLPGQAufl9ZSXOJjZUkNxSU17KhqPGRtnMtO3/REeqclkNMtnu77gkz3JA9ZKXH08MbRLcEVnTdjQiFaysoI7NxFYF0Rgf+3jZJt2whs3UqgpARCoSP21lJRAQo6IiIiciyqGwKs3V3L6l0+1pXWsna3j617Gw45b6ZvegLDenoZ3tNLQY6X/KwkMpM9rSb/mmAQ/6ZNNK1aREtZGc3V1eyqriFUVUWwrIxgaSkEDz/J2J6UhKd/f9wD+uPu0xdXdhbOzEycmVm4sjKxJya2x2noMBR0REREvqZw2LBhTx2LtlSybFsVq3f52Fl96HvPZCZ7OKVXN07N9TIitxun9OyGN8FFuLGRlooKWiq20fJFFTVVlbRUVtFSuRf/+g00r1uH8fuP3IjLhTsnB1fv3rjz+uLu2xdPXh7uvDycWVmdetXUiVLQEREROUbGGDaV17P4i8p9r6qD7g4MkZGa4dmJjIwLMNjWQK6/hriqrbRs3UvosypaqqrYW1lJWUUF4YaGo35fe3Iy8cMLcPXujTMtDUe3VBxpaTi7d8fdOxdnZiY2h6M9DrnTU9ARERE5QEsozN76ADuqGimpamRHVSMb99SxdGvVQaueElx2zk0znGOvYUB9Kal7Sgiv3kxg27bo/Jj6fa/DscXH4+zeHWdGBo60VJxp6TjSUvHk5RF3yim4+/TBZu8Yj1TobBR0RESk0zLGUNMYpLzOT3VjgLrmFur9QeqaW2gKhAiGwgRChpZQmJawoSVkCIXDhEzkz/6WMIGWMP6WEA3+EJUNkRvrVTcGDjmnxtPip6CpgnNcdZwS2EuvyhJcWzcTrq2N1jR/pd7m8eDKycHVsyeuHj1wdu+OIz0NZ3o6jtTIiIwzszv2xMQufXmpPSnoiIjISWeMoSEQwtcUxNcYpLY5SFMwtC90hPEHQ9T7WyL7m4LUNkUCTGMgRFMgRGMgRE1jgIp6P8HQMazHPk52E6ZPQwWjWioZ0lxBn5rdZOzdSfzePQfVhgGcTjz9++MZNJC4QYPwDByIZ+DAyCUlBRhLKeiIiMgJM8ZQ29xCTWOAmsYg1V/5WN0YpKYxQHmtnz11zZTXRu7k25YBpVuCi7QEN8lxTpLjXCTHOYl3O3A77Lj2vZwOGw67Daf9y48epwOPy06cv4nU7RtJ37aB+I1rsa1fiznM3BlHejqe/Hw8AwYQN2QwcUOG4B4wALvb3WbHI21HQUdERKIjLFX1ASob/NTsG2Wpa26JXg766mhKY6CF6q8EGl9TkNCx3OnuAG6HnZR4FynxThL2BROP04HbaScpzok33kVKnAtvvIukOCcJLgcJbgfxbgfdEtxk7rvrr8d5bBNxTThMqLKSYGlpZMl2cTFNxcX4N2/hq9eqDGBLSCAuPz86OuPJz8czMB9nWtpxH6dYR0FHRCTGhMImOl+lbl9YqW4MRB8jUFHnp7IhEL0stP8VOMzTq49HvMtBaoILb4Kb1AQXqQluuiW46JbgIjM5jqyUyE3vMlPiSEtwE+eyH/LSTjgQIFxfT7ihYd+rBuP3Y+qDmJYWTLAFE2qBcJimUIimUDiyPRCIvIJBwg0NhGqqaamuJlRdE1nCXVqKOcw9Z1y9ehE/ciQJo0YSP3Iknvx8rWSKAQo6IiKdUChs2FXdxOaKOjbtqWdbZSM7qyOrg3ZVN9HyNUZXIHJX3vRED90SIiMpSXHOyOUgj5MEj5NEt4N4d+RjJMDsCzLxkY9xrsMHg5ZwC/WBeuqCddQF91BR00wwHCQYChIMB0lwJXB69uk0LFrEjv/8/tc9NUdnt+PMzMSdm0v8qSOIP/VU4k85BWf37u33PcUyCjoiIh1YKGzYureejXvq2Vxez6byyMcvKurxH2UEJtHtiM5X2T+isv9xAumJkWCSsu/SULcEF+mJHuLdRx/BCIVD1AfrqQ3UUhuooiZQx47qWmr31FIbqKUuUIfP76OyqZLK5koqmyqpaq6iseXQjzzY75SMU3j1kldb3anXFh+PPSkRR0IiNo8n8vBJlxOb04XN4cDmdIDdAQ57ZJvbFZkr43Jhj0/AmZaKIzUNR2oqzvQ0XDk5kQnCMfwQS2lN/6VFRDqIllCYzRX1rNlVy5pdkQc9riutpTFw6GcVuZ12+mUk0j8ziX4ZkWcj5aYl0DstgcxkD05H6/uuGGNoammiPli/L4zspcZfw55GH7U1tTQGG6kP1tMQbKCxpZHmlmaaW5rxh/w0tjRS648Emfrgke4Ic3TxzngSXYl4HB7cDjduuxuX3UW/bv0AiBsyhIHLlmJPSNClIzlhCjoiIhZo8LewvqyOz0tr+by0lrW7Ix8PNUoT73IwMDuZ/MwkBmQmMaB7Ev27J9ItOUR5YxllDWVUNW+nsaWRVfUNLKpuwOf3Ud1cTVVzFVXNVdT4a2gINhAyR37A4/GId8aT7E4mxZ0SfSW7k0nxRP6cFpdGenw66XHppMenk+JOIcmdhMvuOuL72lwuHK4j14gcKwUdEZF2ZoxhS0UDn22r4rPt1SzfXs3WykM/5DHJ42RoTgoFOV6G9UyiZ3oQXJXsrN/BjtodrK/bwbubd7C7eDcNwaM/OuBQ7DY7Sa4kunm64fV48Xq8JLuTSXIlkehKJNGVSIIzgXhXPHGOOOKd8a1Czf6PLofCiHR8CjoiIm2srjnIqp0+VuyoZsWOGlaU1BzyeUiZyR6G9EhhYHYC2en1uBPK8bVsZUvNFlbUbuOtdSX4Q0d+mGOqJ5XsxGzS4tNaBZVkdzLpcemkxaWRGpdKN083klxJJLuTiXfG6yZ20mUo6IiInIDmYIjikhpW7/SxZreP1bt8bN178GiNx2nnlNwUhuaGyUzzgauU3U3b2FS9iTcqtxCoODgIAThsDnom9SQ3JZc+yX3ondKb3sm96Znckx6JPYh3xp+EoxTpvBR0RESO047KRj7aWM5HGypYtKWSpuDB815yujno38uHt1sZQWcJVcEdbK/dxvqKJqg4+D3jnfH09/YnPzWf/NR88rx59EnuQ3ZS9lHntIjI4SnoiIgcg7rmIP9cWcrrn5WwsqSm1b7uKTCgVy1ebzktrp1UBLawvfYLikMhqGz9Pk67kz7JfRiQOoD8bvkMSB3AwG4D6ZncE7tNT6cWaWsKOiIihxEKG5Z8Ucnfl+/indWlNAVD2Jw+3N7t5GT6SEneSyM7KW/azRpjoKb116fHpVOQUcCwjGEM7DaQft360Su5l0ZoRE4iBR0Rka8Ihw0rSqr558pS3l5Vyt6mKpyJm3Ckb6FbyjZCjsh1p0qgsunLr8uMz2RI+hAGpw1mSNoQhmUMIyshS5N+RSymoCMiXZ4xhrW7a/nnyt28vaqUXbXVOJPX4kpdRXKvTWCL3NsmRGRp9qDUQQxNH8qAbgPIT81nQLcBpMenW3sQInJICjoi0mUFQ2FeXbydlxZtZ1vNbpxJn+NM/pyk7M3YbF9OMB6YOpDxOeM5Pft0RmaOJNmdbGHXInI8FHREpEv6dHMFP31nLrsCRTiT1pPUfVer/f29/bkg7wIu7Hshed48i7oUkROloCMiXUYoHOKtTe/z9NJ/ssu/HLu3Fs++fTZsjOg+ggm5E5jYayIDUgdY2quItA0FHRHpEowxzFmxk1+s/Dk4GrC7wIGHs3qeyaS+3+Dsnmdrno1IDDrumzZ8/PHHXHrppeTk5GCz2XjzzTdb7TfG8NBDD5GTk0N8fDwTJ05k7dq1rWr8fj+33347GRkZJCYmMnXqVHbu3Nmqprq6msLCQrxeL16vl8LCQmpqalrV7Nixg0svvZTExEQyMjK44447CAQOfXdREem6Kuv93PxKEff8bQ3+6tNJCUzggZG/Y+m1C5k16fdcNuAyhRyRGHXcQaehoYERI0Ywa9asQ+5/7LHHePzxx5k1axbLli0jOzub888/n7q6umjN9OnTmTNnDrNnz2bBggXU19czZcoUQqEvJ/9NmzaN4uJi5s6dy9y5cykuLqawsDC6PxQKcckll9DQ0MCCBQuYPXs2f//737n77ruP95BEJIb9e20ZFzz5Mf9euweXw8ado+5k/vf/wLRTJuN2uK1uT0TamzkBgJkzZ07083A4bLKzs82jjz4a3dbc3Gy8Xq955plnjDHG1NTUGJfLZWbPnh2t2bVrl7Hb7Wbu3LnGGGPWrVtnALN48eJozaJFiwxg1q9fb4wx5p133jF2u93s2rUrWvPXv/7VeDwe4/P5jql/n89ngGOuF5HOY3dNo7nhpWWmz31vmz73vW0mPz7frNlVY3VbItIGjuf3d5veb3zr1q2UlZUxefLk6DaPx8OECRNYuHAhAEVFRQSDwVY1OTk5FBQURGsWLVqE1+tlzJgx0ZqxY8fi9Xpb1RQUFJCTkxOtueCCC/D7/RQVFR2yP7/fT21tbauXiMSWUNjw4qdbOf/xj3l33R6cdhs/mNift24/k2E5XqvbE5GTrE0nI5eVlQGQlZXVantWVhbbt2+P1rjdblJTUw+q2f/1ZWVlZGZmHvT+mZmZrWoO/D6pqam43e5ozYFmzpzJL37xi69xZCLSGWwoq+Pev6+KPotqZO9uzLx8OIMzk8Cu50iJdEXt8jf/wFueG2OOehv0A2sOVf91ar7qgQcewOfzRV8lJSVH7ElEOodAS5gn39vIlD9+wsqSGpI9Tn75zWH8/fsjGPz5n+DP50Goxeo2RcQCbTqik52dDURGW3r06BHdXl5eHh19yc7OJhAIUF1d3WpUp7y8nPHjx0dr9uzZc9D7V1RUtHqfJUuWtNpfXV1NMBg8aKRnP4/Hg8fjOeQ+EemcVpbUcO//rWLDnsiCh0lDMvnVN4eRve1N+NPDUFcaKVz/Txj2LesaFRFLtOmITl5eHtnZ2cybNy+6LRAIMH/+/GiIGT16NC6Xq1VNaWkpa9asidaMGzcOn8/H0qVLozVLlizB5/O1qlmzZg2lpaXRmnfffRePx8Po0aPb8rBEpAMKhw1Pf7SFy59eyIY9daQluvnD1SN5fkKA7NcvhDd/EAk53frAt1+CoZdZ3bKIWOC4R3Tq6+vZvHlz9POtW7dSXFxMWloavXv3Zvr06cyYMYP8/Hzy8/OZMWMGCQkJTJs2DQCv18v111/P3XffTXp6Omlpadxzzz0MHz6cSZMmATBkyBAuvPBCbrjhBp599lkAbrzxRqZMmcKgQYMAmDx5MkOHDqWwsJDf/OY3VFVVcc8993DDDTeQkpJywidGRDquvfV+7vrflXy8MfIk8UtO6cGvLuhF6qe/hOX/EynypMA598AZN4ErzsJuRcRSx7uk68MPPzTAQa/rrrvOGBNZYv7ggw+a7Oxs4/F4zDnnnGNWr17d6j2amprMbbfdZtLS0kx8fLyZMmWK2bFjR6uayspKc80115jk5GSTnJxsrrnmGlNdXd2qZvv27eaSSy4x8fHxJi0tzdx2222mubn5mI9Fy8tFOp9PN1WY0341z/S5720z6KfvmL8u3mbCa+YY85t8Yx5MibzeusOY+gqrWxWRdnI8v79txhhjYc6yVG1tLV6vF5/Pp1EgkU7gf5eVcP8bqwgbGJiVxFNX9GfAwvtg/duRgoyBcOnvoc94axsVkXZ1PL+/9awrEenwjDE8+/EXPPqv9QB8a2RPZp5pJ+7vU6B6K9hdcPZdcPbd4NSCAxH5koKOiHRo4bBh5r8+5/lPtgJw04R+3J+zCtuLd0JLE3TrDVe+DDmnWtuoiHRICjoi0mH5moI8+I81vFm8G4CHJ2Xy3ebnYc5fIgX9z4Mr/gwJaRZ2KSIdmYKOiHQ4LaEwf126g8fnbaS6MUgPezX/M2gx+Yv/LzKKA3DOj2DiA2B3WNusiHRoCjoi0qF8sqmCh/+5jk3l9STTyBMpb/LNlnexbw1ECnJGwXk/g/7nWtuoiHQKCjoi0iEEQ2FmvrOe//40MhdnavwqZrr/m0R/eaQgdyxM+FHkctVRHikjIrKfgo6IWK7U18Rtr62gaHs1qdTyYo83GFH9LviBtH5wyePQb6ICjogcNwUdEbHUgk17uWP2CqoaAoyN28ZLnsfxVO8Fmx3G3QoTfwzuBKvbFJFOSkFHRCzz1srdTJ+9grCBazM28bD/19j9jdB9MHzzKeil59aJyIlR0BERS3ywfg93vV5M2MCv+q3jmrJHsYVboN834KqXwZNsdYsiEgMUdETkpFv8RSU/eGU5LeEwT/T+lG/tfiqyY/i3IyM5Tre1DYpIzFDQEZGTatXOGv7rpc8YFNrEb73/y8Dy1ZEdY2+Fyb8Cu93aBkUkpijoiMhJs2DTXh557d/8Mvwq3/J8GllV5YyH834OY3+gVVUi0uYUdESk3dU1B3n0/60hZfkzzHH+nThHMLJjxNVw7s/A29PaBkUkZinoiEi7+mRTBS/+7Q3ubv4TQ13bAQjljsdx0QzIGWlxdyIS6xR0RKRdBENhHn97OenLfstzjrk47Iaguxuuix/FMeI7ukwlIieFgo6ItLnd1Y28/MIsCn1Pk+OsAqBl2H/guuhRSOpucXci0pUo6IhIm1r82TJa3v4R97ECbNCU0JP4y57EOXCy1a2JSBekoCMibSK4dyur/vYII8vexGMLEsRJ4+m34T3/Pj3CQUQso6AjIiemdCUNHz5O3Ma3GE0YbLAlaTS9rn0Kb/Zgq7sTkS5OQUdEjp+/DtbOgeUvw86lJO7bvIARuM7+IWPOvUyTjUWkQ1DQEZFjt3czfPoErJkDwQYAWoydt8Nj+aT7NH5YeAW9UnWZSkQ6DgUdETk6fx18/BtY9BSEIzf722qyeb1lIm+ac7hiwmh+PWkgToce3yAiHYuCjogcnjGw+m/w7s+gvgyAT8xI/hC4lGVmEOcNzuLliwaTn6UnjYtIx6SgIyKHFmyCN26Ez98CYLvJ4hfBQj4Ij2JojxRevWQIZw7IsLhJEZEjU9ARkYM1VBL+61XYdy7Db5z8vuVy/hK6mIE9u/OnCf25qCAbu12TjUWk41PQEZHWqr4g8NLluH1b8ZkEbgjcjav/WfxlwgDOHJCOTaupRKQTUdARkSiz5SP8r/8ncYEqdpoM7rT/mJuuvYTJw7Ktbk1E5GtR0BERKFmG+eBX2LZ+RBywJtyXZ3rO5KmrzyUrJc7q7kREvjYFHZGurGIjzPsZbJyLDQgYB6+HzyU48ef84RvDNQ9HRDo9BR2RrmrrxzD7GvDXEsbO31rO4Y+hb/HD/5jEFaN7Wd2diEibUNAR6YrWvAFzboJQgD3dRvGdPdew1fTg51OGKuSISExR0BHpahY/A3PvBwzbsyYxefu1+HFz53n5fP+sPKu7ExFpUwo6Il1FQyV88EsoegGAdxMv5ebtVxHGzvfG92X6pHyLGxQRaXsKOiKxLtAIi5+CT38P/loAfhf6Dn+svBSP08EPzx/IjWf30/1xRCQmKeiIxKqWABS/CvN/DXWlAGx19uenjVfyaXg4Zw3I4JFvFdAnPdHiRkVE2o+CjkisCTbDipdhwZNQuxMA483lL+5reaRkGEkeN7+bOozLR/XUKI6IxDwFHZFY0VQTCTgLZ0WfNE5SNubMO/jZrrG88lkZbqed5687jbH90i1tVUTkZFHQEensytfD0udg5V8h2BjZltILzpoOIwv5/fwdvPLZJmw2+P1VpyrkiEiXoqAj0hmFw7D5vcgk4y8+/HJ75lAYczOMuBrjcPHSwm08+d4mAB7+ZgEXDe9hUcMiItZQ0BHpTAINUPwaLHkGKjdHttnsMOjiSMDpexbYbOyt9/OTOUX8e+0eAG4/dwCFY/tY2LiIiDUUdEQ6A2Mil6be/Rk07o1s86TAqO/CGTdC6pchZu6aUn48Zw1VDQFcDhvTJw3klon9LWpcRMRaCjoiHV355/D2XbBjYeTz1DwYewucejV4kqNlFXV+fvn2Ot5auRuAwdnJ/O7KEQzL8VrRtYhIh6CgI9JRBRpg/mOwaBaEW8CVABPug3G3gsMVLQuFDa8t2c5j/95AXXMLdhv8YGJ/7jgvH4/TYeEBiIhYT0FHpCPaNA/+311QsyPy+aBL4KJHoVvvVmWrd/r46ZurWbnTB8Dwnl4e+VYBp/TqdpIbFhHpmBR0RDqSurLIAzfXzol8ntILLv4NDL64VVlJVSO/e3cDbxZHLlMle5z86MJBXDOmDw67bgIoIrKfgo5IRxAOw4r/gXd/Dn5fZCXV2Ftg4gPgSYqWVTUE+OMHm3hl8XaCIQPAZafm8OOLh5CZEmdV9yIiHZaCjojV9m6Gf94J2xdEPs8ZCZf+HnqMiJY0B0O88Ok2nvpwM3X+FgDOzs/gvgsHU9BTk41FRA5HQUfEKsZEnij+4QwI+SOTjc/9aeR+OPbIJOJw2PDWyt385t8b2FXTBMCwnBTuv2gwZ+d3t7J7EZFOwd7Wb9jS0sJPf/pT8vLyiI+Pp1+/fjz88MOEw+FojTGGhx56iJycHOLj45k4cSJr165t9T5+v5/bb7+djIwMEhMTmTp1Kjt37mxVU11dTWFhIV6vF6/XS2FhITU1NW19SCLt49Pfw3sPRkJOv2/ALYsiK6r2hZwdlY186+mFTH+9mF01TfTwxvH4lSP4521nKeSIiByjNg86v/71r3nmmWeYNWsWn3/+OY899hi/+c1v+OMf/xiteeyxx3j88ceZNWsWy5YtIzs7m/PPP5+6urpozfTp05kzZw6zZ89mwYIF1NfXM2XKFEKhULRm2rRpFBcXM3fuXObOnUtxcTGFhYVtfUgibW/HEnj/4cifz/8lFM6B1L7R3R+uL2fKHz9hZUkNSR4nP7pgEB/eM5HLR/XCrsnGIiLHzGaMMW35hlOmTCErK4u//OUv0W1XXHEFCQkJvPzyyxhjyMnJYfr06dx3331AZPQmKyuLX//619x00034fD66d+/Oyy+/zFVXXQXA7t27yc3N5Z133uGCCy7g888/Z+jQoSxevJgxY8YAsHjxYsaNG8f69esZNGjQUXutra3F6/Xi8/lISUlpy9MgcniNVfDsOeArgYL/gCv+DLZIeAmHDb9/fxN/+GATxsCpud14+tpR9PDGW9y0iEjHcTy/v9t8ROess87i/fffZ+PGjQCsXLmSBQsWcPHFkeWxW7dupaysjMmTJ0e/xuPxMGHCBBYujNz5taioiGAw2KomJyeHgoKCaM2iRYvwer3RkAMwduxYvF5vtOZAfr+f2traVi+Rk8oY+MetkZCT1g+mPBENOb6mINe/tIzfvx8JOYVj+/D6TWMVckRETkCbT0a+77778Pl8DB48GIfDQSgU4pFHHuHqq68GoKysDICsrKxWX5eVlcX27dujNW63m9TU1INq9n99WVkZmZmZB33/zMzMaM2BZs6cyS9+8YsTO0CRE7HkGdjwDjjc8O0XIS7yfyIb99Rx4/98xrbKRjxOOzO+NZwrRveytlcRkRjQ5iM6r7/+Oq+88gqvvfYay5cv56WXXuK3v/0tL730Uqs6m631PANjzEHbDnRgzaHqj/Q+DzzwAD6fL/oqKSk51sMSOXGr/hZ5KCfABTOiy8fnrinlW3/6lG2VjfTsFs/ffzBeIUdEpI20+YjOj370I+6//36+853vADB8+HC2b9/OzJkzue6668jOzgYiIzI9evSIfl15eXl0lCc7O5tAIEB1dXWrUZ3y8nLGjx8frdmzZ89B37+iouKg0aL9PB4PHo+nbQ5U5Fg118I7P4JVsyOfD70MTv8vwmHDk+9t5A8fbAZgXL90/nTNKNIS3db1KiISY9p8RKexsRG7vfXbOhyO6PLyvLw8srOzmTdvXnR/IBBg/vz50RAzevRoXC5Xq5rS0lLWrFkTrRk3bhw+n4+lS5dGa5YsWYLP54vWiFhu52fw7NmRkGOzR+50fMVfaAiEuPmVomjI+f6Zebx8/RkKOSIibazNR3QuvfRSHnnkEXr37s2wYcNYsWIFjz/+ON///veByOWm6dOnM2PGDPLz88nPz2fGjBkkJCQwbdo0ALxeL9dffz1333036enppKWlcc899zB8+HAmTZoEwJAhQ7jwwgu54YYbePbZZwG48cYbmTJlyjGtuBJpdxvmwuxpYELg7Q1XPA+9x7Krpon/eukzPi+txe2wM/NyzccREWkvbR50/vjHP/Kzn/2MW265hfLycnJycrjpppv4+c9/Hq259957aWpq4pZbbqG6upoxY8bw7rvvkpycHK154okncDqdXHnllTQ1NXHeeefx4osv4nA4ojWvvvoqd9xxR3R11tSpU5k1a1ZbH5LI8avZAXNujIScIZfC1FkQ342i7dXc9HIRe+v9ZCS5ebbwNEb3ST36+4mIyNfS5vfR6Ux0Hx1pF6EgvHAR7FwGPUfDf84Fp5uVJTV8+9lFBFrCDOmRwp+vO42e3bR0XETkeB3P728960qkrb3/i0jIifPCf/w3ON2Ew4YH31pLoCXMOQO78/Q1o0j06K+fiEh7a/PJyCJd2oa5sHDf406++VT0sQ7/XLWb4pIaEtwOfvsfpyjkiIicJAo6Im3FtxPevDny5zE3w5ApADQHQ/z6X+sBuGVifzJT4qzqUESky1HQEWkr7z8MTdXQ41Q4/+Ho5j9/8gW7fc3keOP4r7P7WdefiEgXpKAj0hYqNsCq/438+dInwRm5MWV5XTNPfbQFgPsuGkycy3GYNxARkfagoCPSFj6aCRgYPAVyRkY3/+7fG2kMhDg1txtTR+RY15+ISBeloCNyosrWwNo5kT9PfCC6edm2Kv63KPI8tZ9NGXLUZ7mJiEjbU9AROVEfzYx8HHoZZBcAsGhLJdf991KMgakjchjdJ826/kREujCtcRU5EbuLYf3bgC06mvPRhnJuerkIf0uYswZk8OgVwy1tUUSkK1PQETkRH86IfBz+bcgczNw1Zdz+1+UEQ4ZJQzKZNW2UJiCLiFhIQUfk69r5GWz6N9gcMPF+PtlUwa2vLScUNlxySg+evOpUXA5dHRYRsZL+FRb5uhbte4DsiO/Q0i2Ph95aSyhs+OapOfzhOyMVckREOgD9SyzyddRXwOdvR/489gfMXlbClooG0hLd/PKyAhx2rbASEekIFHREvo6Vr0E4CD1HU586hCff2wjAHecOICXOZXFzIiKyn4KOyPEyBopeivx51HU8N38Le+sD5GUkMm1MH2t7ExGRVhR0RI7XtgVQtQXcSezpPYXnPvkCgPsuHITbqb9SIiIdif5VFjleRS9GPg7/Nr+bv5PmYJjT+qRywbBsS9sSEZGDKeiIHI+GSvj8LQC29v02fyvaCcADF+sRDyIiHZGCjsjxWPlXCAWgxwhmfZ6EMXBRQTaj+6Ra3ZmIiByCgo7IsTIGlkcmIftHfJd/rSkF4L/OzrOyKxEROQIFHZFjtWMR7N0IrkT+xVk0BkLkZSQyqrdGc0REOioFHZFjVfxa5GPB5cxeVQ3AFaN6am6OiEgHpqAjcizCIdjwLwDK+17K4i+qsNngW6N6WdyYiIgciYKOyLHYVQSNe8Hj5fU9vQEY3z+dnt3iLW5MRESOREFH5FhseAcAkz+JvxWXAfAfozWaIyLS0SnoiByLfZettqSew46qRhLdDt0gUESkE1DQETmayi1QsR7sTl7Zmw/AJaf0IMHttLgxERE5GgUdkaPZOBeAUO54/m9dPQBXaBKyiEinoKAjcjT7LlutTT6Ten8LvdMSOL1vmsVNiYjIsVDQETmSxirYvhCA2b6hAFw2sid2u+6dIyLSGSjoiBzJ5vfAhAh3H8I/trsBOH9IlsVNiYjIsVLQETmSfcvKd2dNpCEQIiPJzbCcFIubEhGRY6WgI3I4LQHY9B4A75vRAJyT312XrUREOhEFHZHD2b4AAnWQmMnsnd0BmDCou8VNiYjI8VDQETmcDZFl5Y155/P5ngZsNjg7X0FHRKQzUdAROZwdkdVWK9wjARjRqxtpiW4rOxIRkeOkoCNyKIFG2LMOgHeqIjcHnDBQozkiIp2Ngo7IoZQWgwlhkrJ5a1tk8vFEzc8REel0FHREDmXnZwBUp55CXXOI1AQXp/TqZm1PIiJy3BR0RA5lVyTorLYNACKTkB1aVi4i0uko6Igcys4iAOZWa36OiEhnpqAjcqC6MqjdicHGWxWRxz2co6AjItIpKeiIHGjf/Bxfcj4NxFPQM4XuyR6LmxIRka9DQUfkQPvm56yz5wO6bCUi0pkp6IgcaN+Izvu1uUDk+VYiItI5KeiIfFU4BLtXALCguS/xLgcje6da3JSIiHxdCjoiX1WxAQL1BBwJbDK9OCMvDbdTf01ERDor/Qsu8lU7lwGwxZlPGDtnDciwuCERETkR7RJ0du3axbXXXkt6ejoJCQmceuqpFBUVRfcbY3jooYfIyckhPj6eiRMnsnbt2lbv4ff7uf3228nIyCAxMZGpU6eyc+fOVjXV1dUUFhbi9Xrxer0UFhZSU1PTHockXcW+icgLmvsCMH5AuoXNiIjIiWrzoFNdXc2ZZ56Jy+XiX//6F+vWreN3v/sd3bp1i9Y89thjPP7448yaNYtly5aRnZ3N+eefT11dXbRm+vTpzJkzh9mzZ7NgwQLq6+uZMmUKoVAoWjNt2jSKi4uZO3cuc+fOpbi4mMLCwrY+JOlK9t0o8LNgP9IS3QzJTrG4IRERORE2Y4xpyze8//77+fTTT/nkk08Oud8YQ05ODtOnT+e+++4DIqM3WVlZ/PrXv+amm27C5/PRvXt3Xn75Za666ioAdu/eTW5uLu+88w4XXHABn3/+OUOHDmXx4sWMGTMGgMWLFzNu3DjWr1/PoEGDjtprbW0tXq8Xn89HSop+oXV5/np4NBdMmDOa/8QZpwxl1rRRVnclIiIHOJ7f320+ovPWW29x2mmn8e1vf5vMzExGjhzJ888/H92/detWysrKmDx5cnSbx+NhwoQJLFy4EICioiKCwWCrmpycHAoKCqI1ixYtwuv1RkMOwNixY/F6vdGaA/n9fmpra1u9RKJ2rwATpsLenXJSOVPzc0REOr02DzpffPEFTz/9NPn5+fz73//m5ptv5o477uB//ud/ACgrKwMgKyur1ddlZWVF95WVleF2u0lNTT1iTWZm5kHfPzMzM1pzoJkzZ0bn83i9XnJzc0/sYCW27Juf81lLPwBNRBYRiQFtHnTC4TCjRo1ixowZjBw5kptuuokbbriBp59+ulWdzdb6SdDGmIO2HejAmkPVH+l9HnjgAXw+X/RVUlJyrIclXcG+GwUuD/Wnd1oCuWkJFjckIiInqs2DTo8ePRg6dGirbUOGDGHHjh0AZGdnAxw06lJeXh4d5cnOziYQCFBdXX3Emj179hz0/SsqKg4aLdrP4/GQkpLS6iUCgDHRpeXF4QGcqdVWIiIxoc2DzplnnsmGDRtabdu4cSN9+vQBIC8vj+zsbObNmxfdHwgEmD9/PuPHjwdg9OjRuFyuVjWlpaWsWbMmWjNu3Dh8Ph9Lly6N1ixZsgSfzxetETlmvhKo30MLDlaZfpqfIyISI5xt/YY//OEPGT9+PDNmzODKK69k6dKlPPfcczz33HNA5HLT9OnTmTFjBvn5+eTn5zNjxgwSEhKYNm0aAF6vl+uvv567776b9PR00tLSuOeeexg+fDiTJk0CIqNEF154ITfccAPPPvssADfeeCNTpkw5phVXIq3sG81ZG+6DHzfj+yvoiIjEgjYPOqeffjpz5szhgQce4OGHHyYvL48nn3ySa665Jlpz77330tTUxC233EJ1dTVjxozh3XffJTk5OVrzxBNP4HQ6ufLKK2lqauK8887jxRdfxOFwRGteffVV7rjjjujqrKlTpzJr1qy2PiTpCkoiQWdFeABDe6SQlui2uCEREWkLbX4fnc5E99GRqOfPg12fcUfgVrLPKuTHFw+xuiMRETkMS++jI9LpBJsxpSsBWGEGcHa+LluJiMQKBR2RslXYwkEqTAo1nhzG5GnFlYhIrFDQESmJrNwrDudz7uAs3E79tRARiRX6F126PLNvxdXycD6Th2Zb3I2IiLQlBR3p8lq2LwFgtS2fCYO6W9yNiIi0JQUd6dp8u3A1lBIyNhLzTifJ0+Z3XBAREQsp6EjXtu+y1XrTm4nD8yxuRkRE2pqCjnRp9V8sAmC5yee8IZkWdyMiIm1NQUe6tMYtiwGoTh1BZnKcxd2IiEhbU9CRrqslQLeatQB0H3K2xc2IiEh7UNCRLqt++wrcBKkySYw97XSr2xERkXagoCNd1tbiDwHY6BpMXvcki7sREZH2oKAjXZZ/W+T+OcEeoy3uRERE2ouCjnRJobAhp24VANnDzrG4GxERaS8KOtIlbdy0nhz2EsJG3ggFHRGRWKWgI13SjpWR+Tm73P1xxqdY3I2IiLQXBR3pksyOyPychqzTLO5ERETak4KOdDnNwRA9983PSR2sy1YiIrFMQUe6nOWbdzGEbQBkaSKyiEhMU9CRLmfbyo9x2sLUOLtj65ZrdTsiItKOFHSkywltjzzfql7zc0REYp6CjnQp1Q0BetVH5ud4B55lcTciItLeFHSkS1m0pYJR9k0AJOcr6IiIxDoFHelSNq35DK+tkYA9HrIKrG5HRETamYKOdCkt2xYBUJ8xAhxOi7sREZH2pqAjXUZJVSN9m9YAkDjgTIu7ERGRk0FBR7qMhVv2Mtq2EQBP3niLuxERkZNBQUe6jJXrN9HXvgeDDXJPt7odERE5CRR0pEswxkTn5zR1GwhxXos7EhGRk0FBR7qEkqomBvjXAeDup8tWIiJdhYKOdAlFO6oYbY/Mz3H2GWtxNyIicrIo6EiX8NnWKgbadkY+6XGqpb2IiMjJo6AjXcIX27aSbGvCYIe0PKvbERGRk0RBR2JeXXOQ8N7IYx/C3lxweizuSEREThYFHYl5K3bUkGcrBcDRPd/ibkRE5GRS0JGYV7S9Ohp0SFfQERHpShR0JOYt31FNP1tZ5JP0/tY2IyIiJ5WCjsS0UNi0unRF+gBrGxIRkZNKQUdi2oayOpr8fvrY9kQ2ZOjSlYhIV6KgIzGtaHsVvWwVuGwhcMZDco7VLYmIyEmkoCMxrfVE5P5g14+8iEhXon/1JaYV7aim/1eDjoiIdCkKOhKzymubKalqop9dS8tFRLoqBR2JWUXbqwEY5qmIbNCKKxGRLkdBR2LW/qCjpeUiIl2Xgo7ErM+2V5NAM97g/hEdzdEREelqFHQkJvmagqze5aPv/jsiJ6RDQpq1TYmIyEmnoCMxacGmvYTChvHdIpevdNlKRKRravegM3PmTGw2G9OnT49uM8bw0EMPkZOTQ3x8PBMnTmTt2rWtvs7v93P77beTkZFBYmIiU6dOZefOna1qqqurKSwsxOv14vV6KSwspKampr0PSTqBjzaUA3B2Wk1kg1ZciYh0Se0adJYtW8Zzzz3HKaec0mr7Y489xuOPP86sWbNYtmwZ2dnZnH/++dTV1UVrpk+fzpw5c5g9ezYLFiygvr6eKVOmEAqFojXTpk2juLiYuXPnMnfuXIqLiyksLGzPQ5JOIBw2fLQxMi9niFvzc0REurJ2Czr19fVcc801PP/886Smpka3G2N48skn+clPfsLll19OQUEBL730Eo2Njbz22msA+Hw+/vKXv/C73/2OSZMmMXLkSF555RVWr17Ne++9B8Dnn3/O3Llz+fOf/8y4ceMYN24czz//PG+//TYbNmxor8OSTmBdaS0VdX4S3Q4y/DsiG3XpSkSkS2q3oHPrrbdyySWXMGnSpFbbt27dSllZGZMnT45u83g8TJgwgYULFwJQVFREMBhsVZOTk0NBQUG0ZtGiRXi9XsaMGROtGTt2LF6vN1pzIL/fT21tbauXxJ79l63G90/HXrklslEP8xQR6ZLaJejMnj2b5cuXM3PmzIP2lZVFVsFkZWW12p6VlRXdV1ZWhtvtbjUSdKiazMzMg94/MzMzWnOgmTNnRufzeL1ecnNzj//gpMP7cEPkctWFeU7w+wAbpOZZ25SIiFiizYNOSUkJd955J6+88gpxcXGHrbPZbK0+N8YctO1AB9Ycqv5I7/PAAw/g8/mir5KSkiN+P+l8ahoDrNgRWWk1Ib0msrFbLrgO/7MoIiKxq82DTlFREeXl5YwePRqn04nT6WT+/Pn84Q9/wOl0RkdyDhx1KS8vj+7Lzs4mEAhQXV19xJo9e/Yc9P0rKioOGi3az+PxkJKS0uolseXjTXsJGxiUlUyGf1+Q1YorEZEuq82Dznnnncfq1aspLi6Ovk477TSuueYaiouL6devH9nZ2cybNy/6NYFAgPnz5zN+/HgARo8ejcvlalVTWlrKmjVrojXjxo3D5/OxdOnSaM2SJUvw+XzRGul6PlofmZ8zcXB3qNwc2aiJyCIiXZazrd8wOTmZgoKCVtsSExNJT0+Pbp8+fTozZswgPz+f/Px8ZsyYQUJCAtOmTQPA6/Vy/fXXc/fdd5Oenk5aWhr33HMPw4cPj05uHjJkCBdeeCE33HADzz77LAA33ngjU6ZMYdCgQW19WNIJhMOG+fuWlU8cmAlLFXRERLq6Ng86x+Lee++lqamJW265herqasaMGcO7775LcnJytOaJJ57A6XRy5ZVX0tTUxHnnnceLL76Iw+GI1rz66qvccccd0dVZU6dOZdasWSf9eKRjWL3LR2VDgCSPk9P6psK/9gWdDAUdEZGuymaMMVY3YZXa2lq8Xi8+n0/zdWLAk+9t5Mn3NnFRQTZPf6cAZuRAuAV+uBa8vaxuT0RE2sjx/P7Ws64kZny0b1n5xEHdoWJDJOTEdYOUntY2JiIillHQkZhQ2xxk1c4aACYMzIQ9ayI7sgrgKLctEBGR2KWgIzFhxY4awgZ6pyWQ7Y2DPfseEptdcOQvFBGRmKagIzHhs21VAJFJyABlqyMfs4ZZ1JGIiHQECjoSE5btCzqn900DY1pfuhIRkS5LQUc6vWAoTHFJDQCn902F+j3QWAk2O2QOsbY5ERGxlIKOdHprd9fSHAzTLcFFv4wkKNs3mpM+AFzx1jYnIiKWUtCRTi86P6dPKna7TZetREQkSkFHOr1l0YnIaZEN0aCjicgiIl2dgo50asYYirZHnnJ/+v4VV9Gl5cMt6kpERDoKBR3p1LZVNrK3PoDbaaegpxda/LB3Y2SnLl2JiHR5CjrSqe2/bDWilxeP0wEV67/y6Icca5sTERHLKehIp/bZQfNzvnLZSo9+EBHp8hR0pFP7bNsB83PKNBFZRES+pKAjnVZlvZ8v9jYAMLr3gSuuND9HREQUdKQT+2zfaquBWUl4E1ytH/2gh3mKiAgKOtKJHTQ/p67sy0c/dB9sYWciItJRKOhIp7XswPk5+ycip+fr0Q8iIgIo6Egn5W8JsXa3D4DT+uyfn7M68lETkUVEZB8FHemUtpQ3EAwZUuKc9ErdN3oTXVqu+TkiIhKhoCOd0qbyOgAGZiVj23+/nOjScj36QUREIhR0pFPauCcSdPKzkiMbgs1fefSDLl2JiEiEgo50Shv31AORpeUAVHwOJgTxaXr0g4iIRCnoSKe0ac+Xl64AKNs3EVmPfhARka9Q0JFOpzkYYntVIwD5+0d09s/Pydb8HBER+ZKCjnQ6m8vrMQZSE1x0T/JENkZHdE6xrjEREelwFHSk09m/4ip//4qrcLj1pSsREZF9FHSk0zloInLNdgjUgcMDGfkWdiYiIh2Ngo50OoediJw5BBwui7oSEZGOSEFHOp0N+++hk3mIFVciIiJfoaAjnUpjoIWSqibgK5euNBFZREQOQ0FHOpXN5ZH5OemJbtIPWnGlER0REWlNQUc6lf0TkaP3z2msgtqdkT/r0Q8iInIABR3pVA47ETm1L8SlWNOUiIh0WAo60qkc9DBPXbYSEZEjUNCRTiV6D51MTUQWEZGjU9CRTqPB38Kumv0rrvaN6OzRM65EROTwFHSk09i0b8VV92QPqYluaPFDxfrITgUdERE5BAUd6TQ2Rici77tsVbEewi0QnwopPS3sTEREOioFHek0NpYd4Y7INptFXYmISEemoCOdxsby/Q/zPDDoaCKyiIgcmoKOdBqbDrx0paXlIiJyFAo60in4moKU+pqBfffQCYehTCuuRETkyBR0pFNYvqMagD7pCXjjXbB7Ofh94E6CjIEWdyciIh2Vgo50Csu2VgFwet+0yIb1/y/yMf98cLgs6kpERDo6BR3pFJbuCzpn5B0QdAZPsagjERHpDBR0pMNrDoZYtdMHwBl902DvZti7AeyuyIiOiIjIYSjoSIe3sqSGQChM92QPfdITYMO+0Zy+Z0Gc19rmRESkQ1PQkQ7vq5etbDbbVy5bXWJhVyIi0hm0edCZOXMmp59+OsnJyWRmZnLZZZexYcOGVjXGGB566CFycnKIj49n4sSJrF27tlWN3+/n9ttvJyMjg8TERKZOncrOnTtb1VRXV1NYWIjX68Xr9VJYWEhNTU1bH5JYbOm2fUGnbxrUl0PJ0siOQRdb2JWIiHQGbR505s+fz6233srixYuZN28eLS0tTJ48mYaGhmjNY489xuOPP86sWbNYtmwZ2dnZnH/++dTV1UVrpk+fzpw5c5g9ezYLFiygvr6eKVOmEAqFojXTpk2juLiYuXPnMnfuXIqLiyksLGzrQxILtYTCLN8eWVp+et802PAvwEDOSPDq+VYiInIUpp2Vl5cbwMyfP98YY0w4HDbZ2dnm0UcfjdY0Nzcbr9drnnnmGWOMMTU1NcblcpnZs2dHa3bt2mXsdruZO3euMcaYdevWGcAsXrw4WrNo0SIDmPXr1x9Tbz6fzwDG5/Od8HFK+1hZUm363Pe2Gf7gXNMSChvzyreNeTDFmPmPWd2aiIhY5Hh+f7f7HB2fL7JaJi0tsix469atlJWVMXny5GiNx+NhwoQJLFy4EICioiKCwWCrmpycHAoKCqI1ixYtwuv1MmbMmGjN2LFj8Xq90ZoD+f1+amtrW72kY9s/P+e0vmk4gg3wxUeRHVpWLiIix6Bdg44xhrvuuouzzjqLgoICAMrKygDIyspqVZuVlRXdV1ZWhtvtJjU19Yg1mZmZB33PzMzMaM2BZs6cGZ3P4/V6yc3NPbEDlHa39Ks3CtzyPoT8kNYPug+2uDMREekM2jXo3HbbbaxatYq//vWvB+2z2WytPjfGHLTtQAfWHKr+SO/zwAMP4PP5oq+SkpJjOQyxiDGGz/bNzzkjL+3L1VaDLoaj/KyIiIhAOwad22+/nbfeeosPP/yQXr16RbdnZ2cDHDTqUl5eHh3lyc7OJhAIUF1dfcSaPXv2HPR9KyoqDhot2s/j8ZCSktLqJR3Xlop6qhoCxLnsDM+Kg41zIzt02UpERI5RmwcdYwy33XYbb7zxBh988AF5eXmt9ufl5ZGdnc28efOi2wKBAPPnz2f8+PEAjB49GpfL1aqmtLSUNWvWRGvGjRuHz+dj6dKl0ZolS5bg8/miNdK5Ldl32Wpkbiruoueh2QcpPSH3DIs7ExGRzsLZ1m9466238tprr/GPf/yD5OTk6MiN1+slPj4em83G9OnTmTFjBvn5+eTn5zNjxgwSEhKYNm1atPb666/n7rvvJj09nbS0NO655x6GDx/OpEmTABgyZAgXXnghN9xwA88++ywAN954I1OmTGHQoEFtfVhigf0P8jy7lx0+/m1k4zd+AnaHhV2JiEhn0uZB5+mnnwZg4sSJrba/8MILfO973wPg3nvvpampiVtuuYXq6mrGjBnDu+++S3JycrT+iSeewOl0cuWVV9LU1MR5553Hiy++iMPx5S+5V199lTvuuCO6Omvq1KnMmjWrrQ9JLLJsW+TS5WW+V8Dvg6zhMOI7FnclIiKdic0YY6xuwiq1tbV4vV58Pp/m63QwJVWNnP3YhwywlzEv7l5s4Rb47j+g30SrWxMREYsdz+9vPetKOqRPN+8F4FfJ/xcJOfkXKOSIiMhxU9CRDumTTXs5w/Y5Y/0LweaA8x+2uiUREemE2nyOjsiJCoUNCzeX84Lr1ciG0ddBpm4QKCIix08jOtLhrN3tY6i/mFPtX2DcSTDxAatbEhGRTkpBRzqcTzbt5UrHfABsp1wFSQc/6kNERORYKOhIh7N8wxdcaF8W+WRUobXNiIhIp6agIx1Kg7+F3rv+Hx5bEH/6UOhxqtUtiYhIJ6agIx3K0q1V/IftQwDcp39XD+8UEZEToqAjHcrGlQsYZt9Oi80VmZ8jIiJyAhR0pEPJ3vw3AMp7ng8JaRZ3IyIinZ2CjnQYZZXVTAx8BEDKuO9Z2ouIiMQGBR3pMLYteB2vrZFye3eShpxvdTsiIhIDFHSkw0jb8DoAm3K+CXb9aIqIyInTbxPpEMIVmxnYuJywsRF/xnVWtyMiIjFCQUc6hJp//QKAjxlJwdACi7sREZFYoaAj1itdRdoXbwHwWf9bcDv1YykiIm1Dv1HEcvXv/ByAf4TGc9mFF1ncjYiIxBIFHbHWtk9JKvmQoHGwot8tDMhMsrojERGJIQo6Yh1jaJobGc15PTSRb19wjsUNiYhIrFHQEetsnEt82Wc0GTcr+t7AsByv1R2JiEiMUdARa4RDBN99CIAXQhdyzeSx1vYjIiIxSUFHrLHhX7gq1+MzCazo/V1G9U61uiMREYlBCjpiicYlLwDw19C5/Od5Iy3uRkREYpWCjpx0e3ZtxbPtAwDWZ3+Tcf3SLe5IRERilYKOnFR76/289dLvcBBmlX0ID3x3Kjabzeq2REQkRinoyEnjawxS+OclnNc8D4Be595AVkqcxV2JiEgsU9CRk6Ix0ML3XlxK4p5l9LOXEXYmkHb6lVa3JSIiMU5BR06K37+/iRU7arjW8wkA9oLLwZNscVciIhLrFHSk3W2pqOe/F2wlkSamOJdENo4qtLYpERHpEhR0pF0ZY/jFP9cRDBnu6bkOZ0sjpA+A3DFWtyYiIl2Ago60q3nr9vDxxgrcDjtXu+ZHNo68FrTSSkRETgIFHWk3zcEQD7+9DoAfj24hruwzsDlgxNUWdyYiIl2F0+oGJHY9M38LO6ubGJQS5Ls7H45sHHQRJGdb25iIiHQZGtGRdrGjspGnP9qCkxZe9T6NvWoLeHNhyhNWtyYiIl2IRnSkzTUGWrj5lSL8LSH+nDabjIrF4E6Cq2dDUqbV7YmISBeiER1pU8YY7v2/VawrreW2hPeY1PgOYIMr/gzZBVa3JyIiXYxGdKRNPT1/C2+v2s31zn9zt3klsnHyLyNzc0RERE4yBR1pMx+uL+e//72UF1zP8A3HSjDA6P+EcbdZ3ZqIiHRRCjrSJrbubeD1v77Av9x/orvNB844mPwrOP2/dM8cERGxjIKOfH0tfti+ELP5fZzL/skztm0AhLsPwf4f/w1ZQ63tT0REujwFnY6gbg+s+b/W24yBcBCCzdCy7xUOHVATBhOKbDeGyLWiIzBH2x9u/Z7hFggFIRTY99EPgUYINkQ+NtdAKIANyAVCxkbjiP8k+dIZ4Io/vnMgIiLSDhR0OgLfTvj3j63u4msJJ2XxTuNQ5jYXMOysqfzg4jOsbklERCRKQacjSEiF4VcevN3hisx1ccaB0wP2A/5z2WyRRyrY7ZGPtq9xt4AD58/YHGB3fPm+Dg843JFeHG5wJ4ArMfLRk8JvFjfy9PwvyE2L57fnjz7+7y8iItKOFHQ6grR+cMXzVndx3LZU1PPnBWsB+PmUYcS5HBZ3JCIi0ppuGChfizGGh95aSzBk+Mag7kwaojsei4hIx6OgI8fNGMPzn3zBJ5v24nbYefDSYdi0hFxERDogXbqS41LTGOBH/7eKeev2AHDLN/rTNyPR4q5EREQOTUFHjtmSLyqZ/noxpb5m3A479180mP88s6/VbYmIiBxWp7909dRTT5GXl0dcXByjR4/mk08+sbqlmBIOGz7dvJc7/rqCq59fTKmvmbyMRN64ZTzfPytPl6xERKRD69QjOq+//jrTp0/nqaee4swzz+TZZ5/loosuYt26dfTu3dvq9jolYwzldX6+qGhg2bYq/lZUQklVU3T/5aN68vA3C0jydOofHRER6SJsxhztdrkd15gxYxg1ahRPP/10dNuQIUO47LLLmDlz5lG/vra2Fq/Xi8/nIyUlpc36KtpexdurSlttMwZCYUMwFCYQCtMSMq3uY2xM5HNjDOEwhNvgP0v0/cyXH8PGRG66bMy+177vbaAhEGJ7ZQONgdZ3YE72OJl6ag7fOb03w3t5T7gvERGRE3E8v7877f+WBwIBioqKuP/++1ttnzx5MgsXLjzk1/j9fvx+f/Tz2tradultQ1k9L3y6rV3e+2Rw2G30So1nQPckLjmlBxcV9CDerXvkiIhI59Npg87evXsJhUJkZWW12p6VlUVZWdkhv2bmzJn84he/aPfehuWkcOs3+h+03Wm343LYcDnsOB127AdMb7HbbJGbHdts2Gibh347bLZW7+uw7/8+ke9ht9mw79vncdnpk5ZAr9QE3M5OP31LRESk8wad/Q6cDGuMOewE2QceeIC77ror+nltbS25ublt3tOI3G6MyO3W5u8rIiIix6fTBp2MjAwcDsdBozfl5eUHjfLs5/F48Hg8J6M9ERER6QA67fUJt9vN6NGjmTdvXqvt8+bNY/z48RZ1JSIiIh1Jpx3RAbjrrrsoLCzktNNOY9y4cTz33HPs2LGDm2++2erWREREpAPo1EHnqquuorKykocffpjS0lIKCgp455136NOnj9WtiYiISAfQqe+jc6La6z46IiIi0n6O5/d3p52jIyIiInI0CjoiIiISsxR0REREJGYp6IiIiEjMUtARERGRmKWgIyIiIjFLQUdERERiloKOiIiIxKxOfWfkE7X/Xom1tbUWdyIiIiLHav/v7WO553GXDjp1dXUA5ObmWtyJiIiIHK+6ujq8Xu8Ra7r0IyDC4TC7d+8mOTkZm83Wpu9dW1tLbm4uJSUlerzE16RzeOJ0Dk+czmHb0Hk8cTqHXzLGUFdXR05ODnb7kWfhdOkRHbvdTq9evdr1e6SkpHT5H8gTpXN44nQOT5zOYdvQeTxxOocRRxvJ2U+TkUVERCRmKeiIiIhIzFLQaScej4cHH3wQj8djdSudls7hidM5PHE6h21D5/HE6Rx+PV16MrKIiIjENo3oiIiISMxS0BEREZGYpaAjIiIiMUtBR0RERGKWgk47eOqpp8jLyyMuLo7Ro0fzySefWN1ShzVz5kxOP/10kpOTyczM5LLLLmPDhg2taowxPPTQQ+Tk5BAfH8/EiRNZu3atRR13fDNnzsRmszF9+vToNp3DY7Nr1y6uvfZa0tPTSUhI4NRTT6WoqCi6X+fxyFpaWvjpT39KXl4e8fHx9OvXj4cffphwOByt0Tls7eOPP+bSSy8lJycHm83Gm2++2Wr/sZwvv9/P7bffTkZGBomJiUydOpWdO3eexKPo4Iy0qdmzZxuXy2Wef/55s27dOnPnnXeaxMREs337dqtb65AuuOAC88ILL5g1a9aY4uJic8kll5jevXub+vr6aM2jjz5qkpOTzd///nezevVqc9VVV5kePXqY2tpaCzvvmJYuXWr69u1rTjnlFHPnnXdGt+scHl1VVZXp06eP+d73vmeWLFlitm7dat577z2zefPmaI3O45H96le/Munp6ebtt982W7duNX/7299MUlKSefLJJ6M1OoetvfPOO+YnP/mJ+fvf/24AM2fOnFb7j+V83XzzzaZnz55m3rx5Zvny5eYb3/iGGTFihGlpaTnJR9MxKei0sTPOOMPcfPPNrbYNHjzY3H///RZ11LmUl5cbwMyfP98YY0w4HDbZ2dnm0UcfjdY0Nzcbr9drnnnmGava7JDq6upMfn6+mTdvnpkwYUI06OgcHpv77rvPnHXWWYfdr/N4dJdccon5/ve/32rb5Zdfbq699lpjjM7h0RwYdI7lfNXU1BiXy2Vmz54drdm1a5ex2+1m7ty5J633jkyXrtpQIBCgqKiIyZMnt9o+efJkFi5caFFXnYvP5wMgLS0NgK1bt1JWVtbqnHo8HiZMmKBzeoBbb72VSy65hEmTJrXarnN4bN566y1OO+00vv3tb5OZmcnIkSN5/vnno/t1Ho/urLPO4v3332fjxo0ArFy5kgULFnDxxRcDOofH61jOV1FREcFgsFVNTk4OBQUFOqf7dOmHera1vXv3EgqFyMrKarU9KyuLsrIyi7rqPIwx3HXXXZx11lkUFBQARM/boc7p9u3bT3qPHdXs2bNZvnw5y5YtO2ifzuGx+eKLL3j66ae56667+PGPf8zSpUu544478Hg8fPe739V5PAb33XcfPp+PwYMH43A4CIVCPPLII1x99dWAfhaP17Gcr7KyMtxuN6mpqQfV6PdOhIJOO7DZbK0+N8YctE0Odtttt7Fq1SoWLFhw0D6d08MrKSnhzjvv5N133yUuLu6wdTqHRxYOhznttNOYMWMGACNHjmTt2rU8/fTTfPe7343W6Twe3uuvv84rr7zCa6+9xrBhwyguLmb69Onk5ORw3XXXRet0Do/P1zlfOqdf0qWrNpSRkYHD4TgoRZeXlx+UyKW122+/nbfeeosPP/yQXr16RbdnZ2cD6JweQVFREeXl5YwePRqn04nT6WT+/Pn84Q9/wOl0Rs+TzuGR9ejRg6FDh7baNmTIEHbs2AHoZ/FY/OhHP+L+++/nO9/5DsOHD6ewsJAf/vCHzJw5E9A5PF7Hcr6ys7MJBAJUV1cftqarU9BpQ263m9GjRzNv3rxW2+fNm8f48eMt6qpjM8Zw22238cYbb/DBBx+Ql5fXan9eXh7Z2dmtzmkgEGD+/Pk6p/ucd955rF69muLi4ujrtNNO45prrqG4uJh+/frpHB6DM88886BbG2zcuJE+ffoA+lk8Fo2NjdjtrX+tOByO6PJyncPjcyzna/To0bhcrlY1paWlrFmzRud0P8umQceo/cvL//KXv5h169aZ6dOnm8TERLNt2zarW+uQfvCDHxiv12s++ugjU1paGn01NjZGax599FHj9XrNG2+8YVavXm2uvvrqLr0c9Vh8ddWVMTqHx2Lp0qXG6XSaRx55xGzatMm8+uqrJiEhwbzyyivRGp3HI7vuuutMz549o8vL33jjDZORkWHuvffeaI3OYWt1dXVmxYoVZsWKFQYwjz/+uFmxYkX0liTHcr5uvvlm06tXL/Pee++Z5cuXm3PPPVfLy79CQacd/OlPfzJ9+vQxbrfbjBo1KrpUWg4GHPL1wgsvRGvC4bB58MEHTXZ2tvF4POacc84xq1evtq7pTuDAoKNzeGz++c9/moKCAuPxeMzgwYPNc88912q/zuOR1dbWmjvvvNP07t3bxMXFmX79+pmf/OQnxu/3R2t0Dlv78MMPD/lv4HXXXWeMObbz1dTUZG677TaTlpZm4uPjzZQpU8yOHTssOJqOyWaMMdaMJYmIiIi0L83RERERkZiloCMiIiIxS0FHREREYpaCjoiIiMQsBR0RERGJWQo6IiIiErMUdERERCRmKeiIiIhIzFLQERERkZiloCMiIiIxS0FHREREYpaCjoiIiMSs/w+eVSvH9rFQeQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train\n",
    "trainPredictPlot = np.empty_like(dataframe)\n",
    "trainPredictPlot[:, :] = np.nan\n",
    "trainPredictPlot[sequence_length:len(Train_Predict)+sequence_length, :] = Train_Predict\n",
    "# val\n",
    "valPredictPlot = np.empty_like(dataframe)\n",
    "valPredictPlot[:, :] = np.nan\n",
    "valPredictPlot[len(Train_Predict)+sequence_length :len(Train_Predict)+len(Val_Predict)+sequence_length, :] = Val_Predict\n",
    "# test\n",
    "testPredictPlot = np.empty_like(dataframe)\n",
    "testPredictPlot[:, :] = np.nan\n",
    "testPredictPlot[len(Train_Predict)+len(Val_Predict)+ sequence_length : len(Train_Predict)+len(Val_Predict)+ sequence_length + len(Test_Predict), :] = Test_Predict\n",
    "\n",
    "plt.plot(dataframe)               # 파란색 : 실제 값\n",
    "plt.plot(trainPredictPlot)      # 주황색 : Training Data 예측값\n",
    "plt.plot(valPredictPlot)        # 초록색 : Validation Data 예측값\n",
    "plt.plot(testPredictPlot)       # 빨간색 : Test Data 예측값\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "178.797px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
